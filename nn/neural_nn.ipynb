{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big picture\n",
    "\n",
    "-> **NN's as functions** - neural networks are a big function made up of a lot of little functions.  The parameters to this function is the weights and biases.  The inputs to this function is usually a vector of all the variables of one training example. \n",
    "\n",
    "-> We can think of this as one big calculus function. We're trying to minimize the cost function. Aka - one big minimiztion function in calculus - we're trying to take the cost and doing our best to minimize it.  We try to find the derivitive of the cost function with respect to every single weight and bias.  How to best figure out how to lower that cost function is to find out how much that weight and bias contribute to the cost.  Then we can appropriately add or subjract the different weights to make sure you're finding your optimal algorithm. The entire goal of backpropagation is to find the derivative of the cost with respect to every different weight and bias so basically how the cost changes when we change a weight or a bias in our algorithm and this can help us tweek them to see how they impact the final cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Calc review\n",
    "- Gradiants\n",
    "- Jacobians\n",
    "- Jocabian chain rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([6], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0. 0.], Predicted: 0.0, Actual: 0.0\n",
      "Input: [0. 1.], Predicted: 1.0, Actual: 1.0\n",
      "Input: [1. 0.], Predicted: 1.0, Actual: 1.0\n",
      "Input: [1. 1.], Predicted: 0.0, Actual: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Testing the trained model\n",
    "with torch.no_grad():  # No need to track gradients for testing\n",
    "    test_outputs = model(inputs)\n",
    "    predicted = (test_outputs > 0.5).float()  # Apply threshold to get binary output\n",
    "    for i, input in enumerate(inputs):\n",
    "        print(f'Input: {input.numpy()}, Predicted: {predicted[i].item()}, Actual: {labels[i].item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 0.4370\n",
      "Epoch [20/300], Loss: 0.3344\n",
      "Epoch [30/300], Loss: 0.1690\n",
      "Epoch [40/300], Loss: 0.1056\n",
      "Epoch [50/300], Loss: 0.1336\n",
      "Epoch [60/300], Loss: 0.0497\n",
      "Epoch [70/300], Loss: 0.1166\n",
      "Epoch [80/300], Loss: 0.0504\n",
      "Epoch [90/300], Loss: 0.0107\n",
      "Epoch [100/300], Loss: 0.0693\n",
      "Epoch [110/300], Loss: 0.0327\n",
      "Epoch [120/300], Loss: 0.0164\n",
      "Epoch [130/300], Loss: 0.0454\n",
      "Epoch [140/300], Loss: 0.1034\n",
      "Epoch [150/300], Loss: 0.0098\n",
      "Epoch [160/300], Loss: 0.1274\n",
      "Epoch [170/300], Loss: 0.0721\n",
      "Epoch [180/300], Loss: 0.0653\n",
      "Epoch [190/300], Loss: 0.0232\n",
      "Epoch [200/300], Loss: 0.0121\n",
      "Epoch [210/300], Loss: 0.0502\n",
      "Epoch [220/300], Loss: 0.0270\n",
      "Epoch [230/300], Loss: 0.0122\n",
      "Epoch [240/300], Loss: 0.1417\n",
      "Epoch [250/300], Loss: 0.0003\n",
      "Epoch [260/300], Loss: 0.0036\n",
      "Epoch [270/300], Loss: 0.0730\n",
      "Epoch [280/300], Loss: 0.0197\n",
      "Epoch [290/300], Loss: 0.0536\n",
      "Epoch [300/300], Loss: 0.0963\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor)\n",
    "    predictions = predictions.round()\n",
    "    accuracy = (predictions.eq(y_tensor).sum() / float(y_tensor.shape[0])).item()\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
