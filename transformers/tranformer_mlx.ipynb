{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple's MLX: Input/Positional Embeddings and Multi-head Self-Attention \n",
    "\n",
    "#### Framework created by Apple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk you through the core concepts and functionalities of MLX, starting with the basics of tokenization and positional encoding. Whether youâ€™re a beginner looking to get started with machine learning on Apple hardware or an experienced practitioner seeking to optimize your workflows, this book provides practical examples and step-by-step instructions to help you harness the full potential of MLX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['what', 'are', 'the', 'advantages', 'and', 'disadvantages', 'of', 'using', 'a', 'unified', 'memory', 'architecture', '?']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenize the sentence\n",
    "sentence = \"What are the advantages and disadvantages of using a unified memory architecture?\"\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a simple vocabulary and convert tokens to indices\n",
    "vocab = defaultdict(lambda: len(vocab))\n",
    "indices = [vocab[token] for token in tokens]\n",
    "print(\"Indices:\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the embedding layer\n",
    "embedding_dim = 64\n",
    "num_embeddings = len(vocab)\n",
    "embedding_layer = nn.Embedding(num_embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Tokens: array([[-0.044703, -0.0250553, 0.213533, ..., -0.1252, 0.279171, -0.0321185],\n",
      "       [-0.0280211, 0.135701, -0.182754, ..., 0.0769782, 0.0877975, 0.109833],\n",
      "       [-0.0382277, -0.0457622, 0.0334275, ..., -0.0309626, -0.228566, 0.217419],\n",
      "       ...,\n",
      "       [-0.107577, 0.0191069, 0.207287, ..., 0.0617455, -0.119649, -0.0756776],\n",
      "       [-0.0589486, 0.0437169, -0.186359, ..., -0.0349201, -0.0430669, 0.049653],\n",
      "       [0.0568032, -0.0073198, -0.122325, ..., 0.154259, -0.0785468, 0.0552094]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Convert indices to MLX array and embed them\n",
    "input_data = mx.array(indices)\n",
    "embedded_tokens = embedding_layer(input_data)\n",
    "print(\"Embedded Tokens:\", embedded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoPE (Rotary Positional Encoding)\n",
    "\n",
    "RoPE applies a rotational transformation to the token embeddings based on their positions in the sequence. This transformation uses sinusoidal functions to create a set of rotation matrices that are applied to the embeddings. The result is a set of positionally encoded embeddings that carry rich relative positional information.\n",
    "\n",
    "In transformer models, understanding the relative positions of tokens within a sequence is crucial for tasks that require contextual understanding, such as language modeling and translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length: 13\n"
     ]
    }
   ],
   "source": [
    "seq_len = embedded_tokens.shape[0]\n",
    "print(\"Sequence Length:\", seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate frequencies for the sinusoidal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Frequencies: [1.0000000e+00 7.4989420e-01 5.6234133e-01 4.2169651e-01 3.1622776e-01\n",
      " 2.3713736e-01 1.7782794e-01 1.3335215e-01 1.0000000e-01 7.4989416e-02\n",
      " 5.6234129e-02 4.2169649e-02 3.1622779e-02 2.3713736e-02 1.7782794e-02\n",
      " 1.3335215e-02 9.9999998e-03 7.4989423e-03 5.6234132e-03 4.2169648e-03\n",
      " 3.1622779e-03 2.3713738e-03 1.7782794e-03 1.3335214e-03 1.0000000e-03\n",
      " 7.4989418e-04 5.6234130e-04 4.2169649e-04 3.1622779e-04 2.3713738e-04\n",
      " 1.7782794e-04 1.3335215e-04]\n",
      "Frequencies: array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [1, 0.749894, 0.562341, ..., 0.000237137, 0.000177828, 0.000133352],\n",
      "       [2, 1.49979, 1.12468, ..., 0.000474275, 0.000355656, 0.000266704],\n",
      "       ...,\n",
      "       [10, 7.49894, 5.62341, ..., 0.00237137, 0.00177828, 0.00133352],\n",
      "       [11, 8.24884, 6.18575, ..., 0.00260851, 0.00195611, 0.00146687],\n",
      "       [12, 8.99873, 6.7481, ..., 0.00284565, 0.00213394, 0.00160023]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inv_freq = 1.0 / (10000 ** (np.arange(0, embedding_dim, 2).astype(np.float32) / embedding_dim))\n",
    "print(\"Inverse Frequencies:\", inv_freq)\n",
    "\n",
    "freqs = mx.array(np.outer(np.arange(seq_len), inv_freq).astype(np.float32))\n",
    "print(\"Frequencies:\", freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate cosine and sine of the frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Positional Encoding: array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [0.540302, 0.731761, 0.846009, ..., 1, 1, 1],\n",
      "       [-0.416147, 0.0709483, 0.431463, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [-0.839072, 0.347628, 0.790132, ..., 0.999997, 0.999998, 0.999999],\n",
      "       [0.0044257, -0.384674, 0.995257, ..., 0.999997, 0.999998, 0.999999],\n",
      "       [0.843854, -0.910606, 0.893862, ..., 0.999996, 0.999998, 0.999999]], dtype=float32)\n",
      "Sine Positional Encoding: array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0.841471, 0.681561, 0.533168, ..., 0.000237137, 0.000177828, 0.000133352],\n",
      "       [0.909297, 0.99748, 0.902131, ..., 0.000474275, 0.000355656, 0.000266704],\n",
      "       ...,\n",
      "       [-0.544021, 0.937633, -0.612937, ..., 0.00237137, 0.00177828, 0.00133352],\n",
      "       [-0.99999, 0.923052, -0.0972765, ..., 0.00260851, 0.00195611, 0.00146687],\n",
      "       [-0.536573, 0.413275, 0.448343, ..., 0.00284564, 0.00213393, 0.00160023]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cos_pos = mx.cos(freqs)\n",
    "sin_pos = mx.sin(freqs)\n",
    "print(\"Cosine Positional Encoding:\", cos_pos)\n",
    "print(\"Sine Positional Encoding:\", sin_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split embedded tokens into even and odd parts\n",
    "\n",
    " Splits the embedded tokens into even and odd parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 (even): (13, 32)\n",
      "x2 (odd): (13, 32)\n"
     ]
    }
   ],
   "source": [
    "x1 = embedded_tokens[:, ::2]\n",
    "x2 = embedded_tokens[:, 1::2]\n",
    "print(\"x1 (even):\", x1.shape)\n",
    "print(\"x2 (odd):\", x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply rotational transformation\n",
    "\n",
    "Applies the rotational transformation to the split parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed x1: (13, 32)\n",
      "Transformed x2: (13, 32)\n"
     ]
    }
   ],
   "source": [
    "x1_new = x1 * cos_pos - x2 * sin_pos\n",
    "x2_new = x1 * sin_pos + x2 * cos_pos\n",
    "print(\"Transformed x1:\", x1_new.shape)\n",
    "print(\"Transformed x2:\", x2_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the new x1 and x2 back together\n",
    "\n",
    "Concatenates the transformed parts back together to get the final positional encoded embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoded Embeddings: array([[-0.044703, 0.213533, -0.120804, ..., 0.247067, -0.1252, -0.0321185],\n",
      "       [-0.129328, -0.157782, -0.0411983, ..., 0.272166, 0.0769838, 0.109845],\n",
      "       [0.0575198, 0.0206218, -0.0881349, ..., -0.0591859, -0.0309707, 0.217358],\n",
      "       ...,\n",
      "       [0.100659, 0.176573, -0.187692, ..., 0.201885, 0.0619683, -0.0758371],\n",
      "       [0.0434556, 0.0926936, -0.161226, ..., -0.00339627, -0.0345849, 0.0495897],\n",
      "       [0.044006, 0.0521202, 0.155811, ..., 0.228939, 0.154033, 0.0550836]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "positional_encoded_embeddings = mx.concatenate([x1_new, x2_new], axis=-1)\n",
    "print(\"Positional Encoded Embeddings:\", positional_encoded_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Self-Attention\n",
    "\n",
    "To enhance the modelâ€™s ability to capture different types of relationships, the transformer employs multi-head self-attention. This technique splits the Query, Key, and Value vectors into multiple smaller sub-vectors, each corresponding to a different attention head. The attention mechanism is applied independently to each head, and the results are concatenated and linearly transformed to produce the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings Shape: (1, 13, 64)\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions\n",
    "num_heads = 8\n",
    "head_dim = embedding_dim // num_heads\n",
    "\n",
    "# Sample input embeddings after positional encoding (from previous section)\n",
    "# For demonstration, we assume `positional_encoded_embeddings` is already defined\n",
    "input_embeddings = positional_encoded_embeddings\n",
    "\n",
    "# Add a batch dimension if missing\n",
    "if len(input_embeddings.shape) == 2:\n",
    "    input_embeddings = input_embeddings[np.newaxis, :, :]\n",
    "\n",
    "print(\"Input Embeddings Shape:\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Transformations\n",
    "\n",
    "This cell applies linear projections to the input embeddings to obtain the Query, Key, and Value matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries Shape: (1, 13, 64)\n",
      "Keys Shape: (1, 13, 64)\n",
      "Values Shape: (1, 13, 64)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Linear Transformations\n",
    "query_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "key_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "value_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "queries = query_proj(input_embeddings)\n",
    "keys = key_proj(input_embeddings)\n",
    "values = value_proj(input_embeddings)\n",
    "\n",
    "print(\"Queries Shape:\", queries.shape)\n",
    "print(\"Keys Shape:\", keys.shape)\n",
    "print(\"Values Shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split for Multi-Head Attention\n",
    "\n",
    "This cell reshapes the Query, Key, and Value matrices to prepare them for multi-head attention. It then transposes these matrices to separate the attention heads and prints their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Queries Shape: (1, 8, 13, 8)\n",
      "Split Keys Shape: (1, 8, 13, 8)\n",
      "Split Values Shape: (1, 8, 13, 8)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split for Multi-Head Attention\n",
    "batch_size, seq_length, _ = queries.shape\n",
    "\n",
    "queries = queries.reshape(batch_size, seq_length, num_heads, head_dim).transpose(0, 2, 1, 3)\n",
    "keys = keys.reshape(batch_size, seq_length, num_heads, head_dim).transpose(0, 2, 1, 3)\n",
    "values = values.reshape(batch_size, seq_length, num_heads, head_dim).transpose(0, 2, 1, 3)\n",
    "\n",
    "print(\"Split Queries Shape:\", queries.shape)\n",
    "print(\"Split Keys Shape:\", keys.shape)\n",
    "print(\"Split Values Shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell calculates the attention scores by performing scaled dot-product attention. It then applies the softmax function to obtain normalized attention weights and computes the final attention output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights Shape: (1, 8, 13, 13)\n",
      "Attention Output Shape: (1, 8, 13, 8)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Scaled Dot-Product Attention\n",
    "dk = head_dim\n",
    "scores = mx.matmul(queries, keys.transpose(0, 1, 3, 2)) / mx.sqrt(mx.array([dk], dtype=queries.dtype))\n",
    "attention_weights = nn.softmax(scores, axis=-1)\n",
    "attention_output = mx.matmul(attention_weights, values)\n",
    "\n",
    "print(\"Attention Weights Shape:\", attention_weights.shape)\n",
    "print(\"Attention Output Shape:\", attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell combines the output from all attention heads back into a single tensor. It reshapes the combined output to match the original embedding dimensions and prints the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Output Shape: (1, 13, 64)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Combine Heads\n",
    "combined_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_length, embedding_dim)\n",
    "print(\"Combined Output Shape:\", combined_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Linear Transformation\n",
    "\n",
    "This cell applies a final linear transformation to the combined output from the attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output after Self-Attention Shape: (1, 13, 64)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Final Linear Transformation\n",
    "output_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "final_output = output_proj(combined_output)\n",
    "print(\"Final Output after Self-Attention Shape:\", final_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we explored the foundational components of the transformer model architecture using the MLX framework, specifically tailored for Apple Silicon. Hereâ€™s a summary of what we covered:\n",
    "\n",
    "1.\tTokenization and Embedding:\n",
    "\n",
    "\t- We began by tokenizing an example sentence into individual tokens using NLTK.\n",
    "\t- Each token was then converted into a unique index using a simple vocabulary.\n",
    "\t- We utilized an embedding layer to convert these token indices into dense vectors, preparing them for \t   further processing.\n",
    "2.\tRotary Position Embedding (RoPE):\n",
    "\n",
    "\t- We applied the RoPE technique to enhance the input embeddings with positional information.\n",
    "\t- RoPE uses a rotational transformation based on sinusoidal functions to incorporate relative positional data directly into the embeddings.\n",
    "\t\n",
    "3. Self-Attention Mechanism:\n",
    "\t- Following positional encoding, we implemented the self-attention mechanism, a core component of transformer models.\n",
    "\t\t- This included:\n",
    "\t\t- Linear transformations to obtain Query, Key, and Value matrices from the input embeddings.\n",
    "\t\t- Splitting these matrices into multiple heads for multi-head attention.\n",
    "\t\t- Calculating attention scores through scaled dot-product attention and applying the softmax function to obtain normalized attention weights.\n",
    "\t\t- Combining the output from all attention heads and applying a final linear transformation to produce the final self-attention output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
