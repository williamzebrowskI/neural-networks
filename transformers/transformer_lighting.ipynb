{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id= {\n",
    "    \"what\": 0,\n",
    "    \"is\": 1,\n",
    "    \"statquest\":\n",
    "    2, \"awesome\": 3,\n",
    "    \"<EOS>\": 4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'what', 1: 'is', 2: 'statquest', 3: 'awesome', 4: '<EOS>'}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [token_to_id[\"what\"], token_to_id[\"is\"], token_to_id[\"statquest\"], token_to_id[\"<EOS>\"], token_to_id[\"awesome\"]],\n",
    "    [token_to_id[\"statquest\"], token_to_id[\"is\"], token_to_id[\"what\"], token_to_id[\"<EOS>\"], token_to_id[\"awesome\"]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([\n",
    "    [token_to_id[\"is\"], token_to_id[\"statquest\"], token_to_id[\"<EOS>\"], token_to_id[\"awesome\"], token_to_id[\"<EOS>\"]],\n",
    "    [token_to_id[\"is\"], token_to_id[\"what\"], token_to_id[\"<EOS>\"], token_to_id[\"awesome\"], token_to_id[\"<EOS>\"]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model=2, max_len=6): \n",
    "        # d_model is short for dimension of the model is the number of world embedding values per token\n",
    "        # max_len is the max number of input tokens our transformer can process (input and output combined)\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) # wCreate a matrix of Position Encoding values\n",
    "        # we start by creating a matrix full of zero's - this will have lax_len rows and d_model columns\n",
    "        # so if max_len = 6 and d_model is 2 our pe would look like this:\n",
    "        # pe = torch([[0., 0.],\n",
    "        #             [0., 0.],\n",
    "        #             [0., 0.]]) \n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1) # Create a column matrix, position, that reps the positions, pos, for each token\n",
    "        # we use torch.arange to create a sequence of numbers between start=0 and end=max_len\n",
    "        # .float() ensures that the numbers are floats\n",
    "        # .unsqueeze(1) turns the sequence of numbers into a column matrix\n",
    "        # For example, is max_len=3, we would get this column matrix:\n",
    "        # tensor{[[0.],\n",
    "        #         [1.],\n",
    "        #         [2]]}\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float() # Create a row matrix that reps 'i', times 2, for each word embedding\n",
    "        # we again use torch.arange to create a aequence of numbers, but this time between 0 and d_model\n",
    "        # by putting step=2 would result in the same sequence of numbers if we multiplied i by 2, saving a little math\n",
    "\n",
    "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "        # each value in position, pos is divided by 10000^2i / d_model so we create div_term to represent the divisor\n",
    "\n",
    "        # Now we do the math:\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # the first term assigns values from the sine function to the matrix 'pe' from above, starting with the first column, 0 and then the\n",
    "        # ::2 means every other column after that.\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # the second term assigns values from the cosine function to the matrix 'e' starting witht eh second column, column 1, and then the\n",
    "        # :: means every other column after that.\n",
    "        self.register_buffer('pe', pe) # Moves 'pe' to a GPU if there is one.\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        # create a forard methen that takes in word embedding values and adds the position encoding values to the world_embedding values\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Self Attension\n",
    "1. Calculate the Query, Key's and Values for each token\n",
    "2. To code this math, we will use matrix notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=2):\n",
    "        # We're passing in d_model, the dimension of the model, or the number of word embedding values per token.\n",
    "        # We need to know the number of word embedding values per token because that defines how large the weight matrices are\n",
    "        # that we use to create the Query, Key, Value.\n",
    "        # So, if the dimenion is 2, that mens each weight matrix needs 2 rows and 2 columns,\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        # We use Nn.Linear to create teh weight matrix and do the math for us\n",
    "        # in_features defines how many rows are in the weight matrix - set to d_model\n",
    "        # out_features defines how many columsn are in the weight matrix - set to d_model\n",
    "        # W_q will give us the untrained weights to need to calculate the Query values\n",
    "        # Since this is a Linear object, it not only does the weights but will do the math when the time comes.\n",
    "        # Same for Keys and Values\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "        # To give us flexibility to input training data in sequenctially or batches, we create some variables to keep track of which indeces are for row and columsn\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "    # Forwad method to valvulate the masked self attension values for each token\n",
    "\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "        # Now we calculate the Queary, Key and Value for each token by passing the encoding to each Lenear() object.\n",
    "        # And now we are able to calulate Attention.\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "        # We start my using torch matmul to multiply q by the transpose of k.\n",
    "        # This calculates te similiaries between the Queries and the Keys which we save into 'sims'.  Softmax(QK^t/sqrt(dk) + M)V \n",
    "        \n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        # Then we scale the similarities by the square root of the number values used in each key. Which \n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "            # The next thing we do is add the mask, if we're using one, to the scale the similarities\n",
    "            # Masking is used by used to prevent early tokens from cheating and looking ahead at later tokens.\n",
    "            # To nderstand how we add a mask using the masked_fill() method:\n",
    "            # Let's imagine the mask is a matrix of True's and Falses\n",
    "            # tensor([[False, True, True],\n",
    "            #         [False, False, True],\n",
    "            #         [False, False, False]])\n",
    "            # And the True values above represent the attention values we want to ignore.  \n",
    "            # So, the masked_fill() method replaces teh Trues with -1e9, which represents -1,000,000,000, an approx of -infinity and replaces the Falses with zero to create the final mask\n",
    "            # that is added to the scaled similaries in scaled_sims.\n",
    "            # tensor([[0, -1e9, -1e9],\n",
    "            #         [0, 0, -1e9],\n",
    "            #         [0, 0, 0]])\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        # The next thing we to do calculate the Attention is run the scaled similarities through a softmax() function.\n",
    "        # Applly the Softmax() function to the scaled similaries determines the percentages of influcence that each token should have on the others which is why\n",
    "        # we store the results in a variable called attention_percents.\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        # Lastly, we used torch.matmul() to multiply the attention percentages by the Values in V and that gives us the final attention scores, stored in attention_scores which we turn.\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class that puts the first 3 steps together (word embedding, positional embedding, attention mechenism) and then we can add the residual connections. Then we'll run those values through a fully connected layer, and then runs them through a softmax to get the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "    # We'll use LightningModule \n",
    "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
    "        # We create a init() method which allots use to speficiy\n",
    "        # num_tokens = the number of tokens in the vocab\n",
    "        # d_model = the nuber of values we want to represent each token\n",
    "        # max_len = the max length of the input plus output.\n",
    "        super().__init__()\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        # We create a Embedding() object and name it we for Word Embedding.\n",
    "        # Embedding needs to know how many tokens are in the vocab, and the number of values we want to represent each token.\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "        # Then we create a Position Encoding object using the class we created earlier and name it pe.\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "        # Then we create our Attention object\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        # Then we create the fully connected layer with nn.Linear().  nn.Linear needs to know how many inputs there are and how many outputs there are. \n",
    "        self.loss =  nn.CrossEntropyLoss()\n",
    "        # Then we create a Loss Function to quantify how well the model performs. In this case, were using CrossEntropyLoss(), because our model has multiple outputs\n",
    "        # and CrossEntroyLoss will apply the softmax function for us.\n",
    "    def forward(self, token_ids):\n",
    "        # Now we put all the pieces together in a forward method. Forward takes an array of token id numbers that will be used as inputs to the transformer.\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        #First, we convert the tokens into Word Embedding values.\n",
    "        position_encoding = self.pe(word_embeddings)\n",
    "        # Then we add the position encoding \n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0))))\n",
    "        #Then we create the mask that will prevent early tokens from looking at late tokens when we calculate Attention.\n",
    "        # We start by creating a matrix of 1s with torch.ones().  For example, if we are passing 4 token_ids in this forward method then the call to \n",
    "        # torch.ones() will make a matrix with 4 rows and 4 columns full of 1s.\n",
    "        # That matrix of ones is then passed to troch.tril(), where it means Lower Triangle, because torch.tril() leaves the values in the Lower Triangle as they are and turns everything else into zeros.\n",
    "        # Ultimatly, we save a matrix with 1s in the lower triangle and 0s in the upper triangle in a variable called mask. \n",
    "        mask = mask == 0\n",
    "        # Then we use 'mask == 0' to convert the 0's into True's and the 1's into Falses. This will be the mask we will use for masked-self-attention.\n",
    "        self_attention_values = self.self_attention(position_encoding,\n",
    "                                                    position_encoding,\n",
    "                                                    position_encoding,\n",
    "                                                    mask=mask)\n",
    "        # Once we have the max, we calculate the attention.  Because the Qeury, Key and Value matrices will all be calculated from the same token encoding, we pass in the same set of\n",
    "        # of position encoded values 3 times for the Queries, Keys and Values. We also pass in the mask, so early tokens cant cheat and look ahead at later tokens.\n",
    "        residual_connection_values = position_encoding + self_attention_values\n",
    "        # Ten we add the resitual connections\n",
    "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "        # Lastly, run everything through a fully connected layer.\n",
    "        # Remmber, the loss fucntion we are using, Cross EntropyLoss(), does the Softmax() for us. So all we have to do is return the output of the fully connected layer.\n",
    "        return fc_layer_output\n",
    "    \n",
    "    # Let's create a method to configure the optimizer we are using\n",
    "    def configure_optimizer(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "        # In this case, we're using Adam, which is like Stochastic Gradient Descent, but a little less stochastic\n",
    "        # and we are passing all of the Weights and Biases in the model that we want to train, which is all of them, to Adam.\n",
    "        # We're setting the learning rate to 0.1 ebcasue it makes training this specific model very fast.  The default value of 0.001 is commonly used.\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Then we create a training_stop() method which takes a batch of training data and an index for that batch.\n",
    "        input_tokens, labels = batch\n",
    "        # We then split the training data into inputs and labels\n",
    "        output = self.forward(input_tokens[0])\n",
    "        # Then we pass the input tokens into the forward() method that we just wrote to comput the output\n",
    "        loss = self.loss(output, labels[0])\n",
    "        # Then we compare the output from the Transformer to the known labele using the loss function\n",
    "        return loss\n",
    "        # And remember, the loss function does the softmax for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model before training it, just to see what it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)\n",
    "# The first thing we do is create amodel from the DcoderOnly Transformer() class that we just created\n",
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"statquest\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "# Then we create an input prompt, in this case, we're using 'What is StateQuest <EOS>\n",
    "input_length = model_input.size(dim=0)\n",
    "# Then we figure ou how many tokens we are using as input\n",
    "# We do this because our super simple model can only handle a total of 6 tokens, aka max_len = 6.\n",
    "# So, keeping track of how many tokens are in the input will tell us how many we can create as output.\n",
    "predictions = model(model_input)\n",
    "# Then we run that through the Transformer, which generates predictions for each token in the input.\n",
    "# This mean, that the model generates a prediction for what should come after the first token, 'What' and for all of the other input tokens.\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "# However, we're really just interestd in what the model predicts will come after the <EOS> token so we use -1 to index the outputs generated by the <EOS token.\n",
    "# The outputs generated by the <EOS> token are an array of output values, one per possible output token. so we use the argmax function to identify the \n",
    "# output with the largest value. Thus, the token with the largest output value will be the first token generated as a response to the input.\n",
    "predicted_ids = predicted_id\n",
    "# Save that token so we can print it out later\n",
    "max_length = 6\n",
    "for i in range(input_length, max_length):\n",
    "    # we then loop to keep generated output tokens until we reach the maxinum number of tokens that our model can generated\n",
    "    if (predicted_ids == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "    # or the model generates the <EOS> token\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    # Each time we generate a new output token, we add it to the input, so that each prediction is made with the full context.\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1:])])\n",
    "    # Then the model predicts the next output token using the full context, which is the input plus the output tokens so far.\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "    print(\"Predicted Tokens: \\n\")\n",
    "    for id in predicted_ids:\n",
    "        print(\"\\t\", id_to_token[id.item()])\n",
    "    # Lastly, we print out the generated tokens after converting them from id numbers to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# First, we created a Lightning Tainer and tell it to only do 30 epochs, which is enough for our simple model and dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Then we pass our model and the dataloader we created earlier to the trainer using the fit() method.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:936\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_callbacks()\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_logging_functions()\n\u001b[0;32m--> 936\u001b[0m \u001b[43m_verify_loop_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# SET UP THE TRAINER\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    941\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: setting up strategy environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:36\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected: Trainer state fn must be set before validating loop configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43m__verify_train_val_loop_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     __verify_manual_optimization_support(trainer, model)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mVALIDATING:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:59\u001b[0m, in \u001b[0;36m__verify_train_val_loop_configuration\u001b[0;34m(trainer, model)\u001b[0m\n\u001b[1;32m     57\u001b[0m has_optimizers \u001b[38;5;241m=\u001b[39m is_overridden(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigure_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_optimizers:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# verify minimum validation requirements\u001b[39;00m\n\u001b[1;32m     65\u001b[0m has_val_loader \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mval_loop\u001b[38;5;241m.\u001b[39m_data_source\u001b[38;5;241m.\u001b[39mis_defined()\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined."
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=60)\n",
    "# First, we created a Lightning Tainer and tell it to only do 30 epochs, which is enough for our simple model and dataset\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "# Then we pass our model and the dataloader we created earlier to the trainer using the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens: \n",
      "\n",
      "\t is\n",
      "\t statquest\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_length, max_length):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# we then loop to keep generated output tokens until we reach the maxinum number of tokens that our model can generated\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (predicted_ids \u001b[38;5;241m==\u001b[39m token_to_id[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<EOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# or the model generates the <EOS> token\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)\n",
    "# The first thing we do is create amodel from the DcoderOnly Transformer() class that we just created\n",
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"statquest\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "# Then we create an input prompt, in this case, we're using 'What is StateQuest <EOS>\n",
    "input_length = model_input.size(dim=0)\n",
    "# Then we figure ou how many tokens we are using as input\n",
    "# We do this because our super simple model can only handle a total of 6 tokens, aka max_len = 6.\n",
    "# So, keeping track of how many tokens are in the input will tell us how many we can create as output.\n",
    "predictions = model(model_input)\n",
    "# Then we run that through the Transformer, which generates predictions for each token in the input.\n",
    "# This mean, that the model generates a prediction for what should come after the first token, 'What' and for all of the other input tokens.\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "# However, we're really just interestd in what the model predicts will come after the <EOS> token so we use -1 to index the outputs generated by the <EOS token.\n",
    "# The outputs generated by the <EOS> token are an array of output values, one per possible output token. so we use the argmax function to identify the \n",
    "# output with the largest value. Thus, the token with the largest output value will be the first token generated as a response to the input.\n",
    "predicted_ids = predicted_id\n",
    "# Save that token so we can print it out later\n",
    "max_length = 6\n",
    "for i in range(input_length, max_length):\n",
    "    # we then loop to keep generated output tokens until we reach the maxinum number of tokens that our model can generated\n",
    "    if (predicted_ids == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "    # or the model generates the <EOS> token\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    # Each time we generate a new output token, we add it to the input, so that each prediction is made with the full context.\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1:])])\n",
    "    # Then the model predicts the next output token using the full context, which is the input plus the output tokens so far.\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "    print(\"Predicted Tokens: \\n\")\n",
    "    for id in predicted_ids:\n",
    "        print(\"\\t\", id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
