{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40b175b",
   "metadata": {},
   "source": [
    "# Decoder-Only Transformer Model for Text Generation\n",
    "\n",
    "This notebook implements a decoder-only Transformer model for text generation, similar to the architecture used in GPT (Generative Pre-trained Transformer). The model is trained on a text dataset, specifically the book \"Pride and Prejudice,\" to learn to generate text in the style of the book. After training, the model can be used to generate text by predicting the next token in a sequence based on the previous tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe25c52",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This cell imports necessary libraries and initializes constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b3de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style, init\n",
    "\n",
    "# Initialize colorama\n",
    "init(autoreset=True)\n",
    "\n",
    "# Constants\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_DECODER_LAYERS = 3\n",
    "MAX_SEQ_LENGTH = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d6b92",
   "metadata": {},
   "source": [
    "# Vocabulary Creation Function\n",
    "\n",
    "This function creates a vocabulary from the input text. It assigns a unique index to each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f7763e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text):\n",
    "    \"\"\"\n",
    "    Create a vocabulary from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping words to unique indices.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    vocab = {word: i+4 for i, word in enumerate(unique_words)}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<unk>'] = 1\n",
    "    vocab['<sos>'] = 2\n",
    "    vocab['<eos>'] = 3\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4ab50",
   "metadata": {},
   "source": [
    "# TextDataset Class\n",
    "\n",
    "This class loads text data and prepares it for training. It creates sequences of fixed length from the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e426dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading text data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath, vocab=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to the text file.\n",
    "            vocab (dict, optional): Predefined vocabulary. If None, a new vocabulary is created.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read().replace('\\n', ' ')\n",
    "\n",
    "        if vocab is None:\n",
    "            self.vocab = create_vocab(text)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        \n",
    "        self.data = [self.vocab.get(word, self.vocab['<unk>']) for word in text.split()]\n",
    "        self.data += [self.vocab['<eos>']] * MAX_SEQ_LENGTH\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data) - MAX_SEQ_LENGTH + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sequence from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sequence.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing the input sequence and the target sequence.\n",
    "        \"\"\"\n",
    "        sequence = self.data[idx:idx+MAX_SEQ_LENGTH]\n",
    "        input_sequence = torch.tensor(sequence[:-1], dtype=torch.long)\n",
    "        target_sequence = torch.tensor(sequence[1:], dtype=torch.long)\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c79c5",
   "metadata": {},
   "source": [
    "# TransformerModel Class\n",
    "\n",
    "This class defines the decoder-only Transformer model for text generation. It includes the embedding layer, positional encoding, Transformer decoder layers, and the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab7dd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder-only Transformer model for text generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "        self.pos_encoder = PositionalEncoding(EMBEDDING_SIZE, MAX_SEQ_LENGTH)\n",
    "        self.transformer_decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=EMBEDDING_SIZE, nhead=NHEAD, dim_feedforward=FFN_HID_DIM\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            self.transformer_decoder_layer, num_layers=NUM_DECODER_LAYERS\n",
    "        )\n",
    "        self.fc_out = nn.Linear(EMBEDDING_SIZE, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence to prevent attending to future tokens.\n",
    "        \n",
    "        Args:\n",
    "            sz (int): The size of the mask.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The generated mask.\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            src (torch.Tensor): The input sequence.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output logits for the next token prediction.\n",
    "        \"\"\"\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(0)).to(src.device)\n",
    "        src = self.embedding(src) * math.sqrt(EMBEDDING_SIZE)\n",
    "        src = self.pos_encoder(src)\n",
    "        # Pass src as memory. In an autoregressive model, this is equivalent to the decoder attending to itself.\n",
    "        output = self.transformer_decoder(tgt=src, memory=src, tgt_mask=src_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding module to add positional information to the embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Initialize the positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            d_model (int): The dimension of the model.\n",
    "            max_len (int): The maximum length of the sequences.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to add positional encoding to the input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The embeddings with added positional encoding.\n",
    "        \"\"\"\n",
    "        return x + self.encoding[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d1347",
   "metadata": {},
   "source": [
    "# PositionalEncoding Class\n",
    "\n",
    "This class adds positional information to the input embeddings to help the model understand the order of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9d055",
   "metadata": {},
   "source": [
    "# Count Parameters Function\n",
    "\n",
    "This function counts the number of trainable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b00803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "    \n",
    "    Returns:\n",
    "        int: The number of trainable parameters.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200ed10",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "\n",
    "This cell prepares the dataset, initializes the model, and trains it. It includes the training loop, loss calculation, and model saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09dafd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,596,278 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  35%|███▌      | 1434/4076 [28:24<52:21,  1.19s/it, loss=6.4197]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src)\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mvocab)), tgt_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/transform_llm/.venv/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transform_llm/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to prepare the dataset, initialize the model, and train it.\n",
    "    \"\"\"\n",
    "    # Prepare dataset\n",
    "    dataset = TextDataset(\"../book.txt\")\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = TransformerModel(len(dataset.vocab))\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = count_parameters(model)\n",
    "    print(f\"The model has {total_params:,} trainable parameters\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch+1}/{NUM_EPOCHS}', leave=True)\n",
    "        for i, (src, tgt) in progress_bar:\n",
    "            src = src.transpose(0, 1)\n",
    "            tgt_output = tgt.transpose(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = criterion(output.view(-1, len(dataset.vocab)), tgt_output.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'transformer_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
