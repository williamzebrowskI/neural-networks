{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Positioning Encoding\n",
    "\n",
    "The order of position of the letters in a word is important to convey the intented meaning.  It's crucial to capture the position of each token to maintain the semantic meaning, as tokens are processed independently and simultaneously.\n",
    "\n",
    "Consider the phrases:\n",
    "\n",
    "\"King and Queen are awesome\"\n",
    "\n",
    "&\n",
    "\n",
    "\"Queen and King are awesome\"\n",
    "\n",
    "These sentences are slightly different but their vector representations of embeddings are identical.  We introduce positional encoding to address this issue.  It incorperates information about the position of each embedding within a sequence.  Positional encoding is added to the input embeddings, enabling the model to diffrentiate between the positions of various elements in the input sequence.\n",
    "\n",
    "Adding positional encoding to our example phrases, you will see that the vector representation would be different than if we did not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef9dda",
   "metadata": {},
   "source": [
    "\n",
    "![Positional Encoding Image](/Users/williamzebrowski/transform_llm/images/cosine-sine.png)\n",
    "\n",
    "Positional encoding consists of a series of sine and consine waves and involves two parameters:\n",
    "\n",
    "1. the `pos` parameter\n",
    "\n",
    "- PE(`pos`, 2i)\n",
    "- PE(`pos`, 2i + 1)\n",
    "\n",
    "..represents the position of the sine wave over time. Similar to the time variable `t` or `x` coordinate in a standard plot: \n",
    "\n",
    "![Position Param Image](/Users/williamzebrowski/transform_llm/images/pos-param.png)\n",
    "\n",
    "2. The parameter `i`, which is the `dimention index`;\n",
    "\n",
    "- PE(pos, `2i`)\n",
    "- PE(pos, `2i + 1`)\n",
    "\n",
    "..effectively generates a unique sine or cosine wave for each embedding, controlling the number of oscillations for each wave. An oscillation refers to a single cycle of a wave, moving from the highest point to the lowest point and back to the highest point. The number of oscillations refers to how many times the wave cycles within the positional encoding. Each sine and cosine wave controls the number of oscillations for each wave, and these waves are added to different dimensions of the word embeddings. This process allows the model to encode positional information within each embedding. Each one of these waves is added to a different dimension in the word embedding.\n",
    "\n",
    "This is part of the design of positional encodings in transformer models, which use sine and cosine functions to generate these values.\n",
    "\n",
    "- For even indices `(0, 2, ...)` - even, the positional encoding uses a sine function.\n",
    "- For odd indices `(1, 3, ...)` - odd, the positional encoding uses a cosine function.\n",
    "\n",
    "where:\n",
    "`pos` is the position of the word in the sentence.\n",
    "`i` is the index within the embedding vector.\n",
    "`d` is the dimensionality of the word embeddings (4 in your example).\n",
    "\n",
    "\n",
    "![Dimension Index Image](/Users/williamzebrowski/transform_llm/images/dim-index.png)\n",
    "\n",
    "Explore further with an example and begin with a sequence of embeddings for the phrase 'Transformers are awesome'.  Each row represents a embedding for a specific word and each column cooresponds with an element in that embedding.\n",
    "\n",
    "Input Embeddings:\n",
    "\n",
    "| Transformers| 0.2 | 0.4 | 0.1 | 0.3 |\n",
    "|-------------|-----|-----|-----|-----|\n",
    "|     are     | 0.5 | 0.2 | 0.7 | 0.9 |\n",
    "|   awesome   | 0.8 | 0.6 | 0.4 | 0.2 |\n",
    "\n",
    "`POS(t)` is the specific position of each word embedding within the sequence.  \n",
    "\n",
    "| <span style=\"color: purple\">POS(t)</span>|\n",
    "|-------|\n",
    "| 0     |\n",
    "| 1     |\n",
    "| 2     |\n",
    "\n",
    "In this example, the sequence length is 3.\n",
    "\n",
    "Each word embedding has a demensionality of 4 and will be differientiated based on whether its indeces are odd or even. \n",
    "\n",
    "| Dimensions | 0   | <span style=\"color: purple\">1</span> | 2   | <span style=\"color: purple\">3</span> |\n",
    "|-------------|-----|-----|-----|-----|\n",
    "\n",
    "\n",
    "Let's add positional encoding to the embedding for `Transformers`:\n",
    "\n",
    "| Transformers| 0.2 | 0.4 | 0.1 | 0.3 |\n",
    "|-------------|-----|-----|-----|-----|\n",
    "\n",
    "If you remember:\n",
    "- For even indices `(0, 2, ...)`, the positional encoding uses a sine function.\n",
    "- For odd indices `(1, 3, ...)`, the positional encoding uses a cosine function.\n",
    "\n",
    "For each dimension in the positional encoding, you introduce the corresponding sine and consine waves.  Thus for `Transformers (pos = 0)` \n",
    "\n",
    "For `i=0`, a sine wave is added, and this pattern is repeated for `i=1`, ensuring that each dimnsion is uniquely represente by its own wave. \n",
    "\n",
    "`i=0` \n",
    "\n",
    "![Position 0 Image](/Users/williamzebrowski/transform_llm/images/tran_pos0.png)\n",
    "\n",
    "`i=1`\n",
    "\n",
    "![Position 1 Image](/Users/williamzebrowski/transform_llm/images/tran_pos1.png)\n",
    "\n",
    "\n",
    "\n",
    "Similiarly, the positional encoding values are calculated for the words \"are\" and \"awesome\"\n",
    "\n",
    "A positional encoding table:\n",
    "\n",
    "| Transformers|   0  |   1   |  0   |   1  |\n",
    "|-------------|------|-------|------|------|\n",
    "|     are     | 0.84 |  0.54 | 0.01 | 0.99 |\n",
    "|   awesome   | 0.90 | -0.41 | 0.02 | 0.99 |\n",
    "\n",
    "\n",
    "The 3 graphs represent positional encoding, one for each dimension of the word embeddings.  For the sequence length of 3, you see only three values for each sine function.\n",
    "\n",
    "![Positional Graph Image](/Users/williamzebrowski/transform_llm/images/pos_graph.png)\n",
    "\n",
    "Let's generate positional encoding with an embedding dimension of 8 to depict a realistic setting:\n",
    "\n",
    "| Transformers|   0  |   1   |  0   |   1  |   0  |   1  |  0   |   1  |\n",
    "|-------------|------|-------|------|------|------|------|------|------|\n",
    "|     are     | 0.84 |  0.54 | 0.01 | 0.99 | 0.03 |   1  |   0  |   1  |\n",
    "|   awesome   | 0.90 | -0.41 | 0.02 | 0.99 | 0.05 |   1  | 0.01 |   1  | \n",
    "|    pos=3    |  -   |   -   |   -  |  -   |  -   |   -   |   -  |  -  |\n",
    "|    pos=4    |  -   |   -   |   -  |  -   |  -   |   -   |   -  |  -  |\n",
    "|    pos=99   |  -1  |  0.04 | 0.02 |  -1  | 0.61 | -0.79 | 0.38 | 0.92|\n",
    "\n",
    "\n",
    "Positional encoding can be conceptualized as a series of vectors, where each vector captures a specific location within the sequence. When this poitional vector is added to its corresponding embedding vector, the combination preservces the positional information, ensuring the elements sequence order is maintained within the resulting vector.  In models such as GPT, positional encodings are not static but rather learnable paramters.  These learning params, represented by tensors, are added to the embedding vector and optimized during training.  \n",
    "\n",
    "Segment embeddings used in certain models, such as BERT, are related to positional encodings, providing additional positional informaiton. You can intergrate segment embeddings into the existing embeddings alongside the positional encodings. \n",
    "\n",
    "1. For instance, if the embedding size is 4, a word might be represented as:\n",
    "\n",
    "`x_w = [0.2, 0.4, 0.1, 0.3]`\n",
    "\n",
    "2. For a position `i` in a sequence, the positional encoding might be:\n",
    "\n",
    "`p_i = [0, 1, 0, 1]`\n",
    "\n",
    "3. Adding positional embeddings to word embeddings.  This produces a new vector that contains both the words semantic meaning and its position in the sequence.\n",
    "\n",
    "Let's perform the addition element-wise:\n",
    "- For the first element: `0.2 + 0 = 0.2`\n",
    "- For the second element: `0.4 + 1 = 1.4`\n",
    "- For the third element: `0.1 + 0 = 0.1`\n",
    "- For the fourth element: `0.3 + 1 = 1.3`\n",
    "\n",
    "`x'_w = x_w + p_i = [0.2, 0.4, 0.1, 0.3] + [0, 1, 0, 1] = [0.2, 1.4, 0.1, 1.3]`\n",
    "\n",
    "4. Learnable positional encoding in GPT models\n",
    "\n",
    "Learnable positional encodings offer flexibility and can adapt to the specific patterns of the dataset, potentially leading to better performance on tasks where the relative position of tokens is particularly important.\n",
    "\n",
    "The learnable positional encoding for position `i` might initailly be:\n",
    "\n",
    "`w_i = [0.02, 0.04, 0.08, 0.06]`\n",
    "\n",
    "5. `Segment Embeddings` are additional embeddings used in models like BERT to distinguish between different segments of text (eg., sentences in a document)\n",
    "\n",
    "Segment embeddings allow the model to distinguish between different parts of the input, such as the question and answer in a question-answering task. By adding a unique embedding to tokens from different segments, the model can learn segment-specific representations.\n",
    "\n",
    "`s_i = [0.05, 0.07, 0.1, 0.02]`\n",
    "\n",
    "6.  Integrating `Segment Embeddings` to the existing word embeddings and positional encodings.\n",
    "\n",
    "The integration of segment embeddings with word embeddings and positional encodings enriches the input representation with multiple facets of information: semantic content (word embeddings), position in the sequence (positional encodings), and role in the context of multiple sequences (segment embeddings). This comprehensive representation enables the model to perform complex reasoning over the input.\n",
    "\n",
    "\n",
    "`x''_w = x'_w + s_i = [0.11, 0.33, 0.77, 0.55] + [0.05, 0.07, 0.1, 0.02] = [0.16, 0.4, 0.87, 0.57]`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f96ad4",
   "metadata": {},
   "source": [
    "Let's see how we can implement positional encoding in PyTorch\n",
    "\n",
    "1. We generate actual embeddings in a input sequence the positional encoder will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552aa8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4098,  0.1499, -0.3744,  0.0252]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab = {'Transformers': 0, 'are': 1, 'awesome': 2}\n",
    "token_indices = [vocab['Transformers']]\n",
    "\n",
    "VOCAB_SIZE = len(vocab)  # Size of the vocabulary\n",
    "EMBEDDING_DIM = 4  # Dimension of the embedding vector\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "# Convert token_indices to a tensor\n",
    "token_indices_tensor = torch.tensor(token_indices, dtype=torch.long)\n",
    "\n",
    "# Generate embeddings for the given indices\n",
    "transformer_token = embedding(token_indices_tensor)\n",
    "\n",
    "print(transformer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6381bc",
   "metadata": {},
   "source": [
    "Next, we bring in the generated embedding for the word 'Transformers' and create the positional embedding by using the sin and cosign algorithms, adding them to the original embedding to create a final positional encoded embedding.\n",
    "\n",
    "We will break down each line that's needed and include comments as reference to what exactly is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34c5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embedding:\n",
      "tensor([[ 0.4098,  0.1499, -0.3744,  0.0252]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Scaling factor:\n",
      "tensor([1.0000, 0.0100])\n",
      "\n",
      "Position:\n",
      "tensor([[0]])\n",
      "\n",
      "Initialize Position Embedding:\n",
      "tensor([[0., 0., 0., 0.]])\n",
      "\n",
      "sine:\n",
      "tensor([[0., 0.]])\n",
      "\n",
      "consine:\n",
      "tensor([[1., 1.]])\n",
      "\n",
      "Positional Encoding:\n",
      "tensor([[0., 1., 0., 1.]])\n",
      "\n",
      "Final Encoded Embedding:\n",
      "tensor([[ 0.4098,  1.1499, -0.3744,  1.0252]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "# Define parameters\n",
    "emb_size = 4\n",
    "seq_len = 1\n",
    "\n",
    "# Let's get the \"Transformer\" word embedding from above:\n",
    "print(f\"\\nWord embedding:\")\n",
    "print(transformer_token)\n",
    "\n",
    "# Calculate the scaling factor for each dimension\n",
    "# This corresponds to 1 / 10000^(2i/d_model) in the formula\n",
    "den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "print(\"\\nScaling factor:\")\n",
    "print(den)\n",
    "\n",
    "# Create a position tensor for the sequence\n",
    "pos = torch.arange(0, seq_len).reshape(seq_len, 1)\n",
    "print(\"\\nPosition:\")\n",
    "print(pos)\n",
    "\n",
    "# Initialize the positional embedding tensor with zeros\n",
    "# This tensor will hold the final positional encodings\n",
    "pos_embedding = torch.zeros((seq_len, emb_size))\n",
    "print(\"\\nInitialize Position Embedding:\")\n",
    "print(pos_embedding)\n",
    "\n",
    "# Fill the even-indexed columns (0, 2, ...) of the positional embedding with sine values\n",
    "# The sine function is applied to the element-wise product of the position and denominator tensors\n",
    "pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "print(\"\\nsine:\")\n",
    "print(pos_embedding[:, 0::2])\n",
    "\n",
    "# Fill the odd-indexed columns (1, 3, ...) of the positional embedding with cosine values\n",
    "# The cosine function is applied to the element-wise product of the position and denominator tensors\n",
    "pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "print(\"\\nconsine:\")\n",
    "print(pos_embedding[:, 1::2])\n",
    "\n",
    "# Add positional encodings to the token embedding\n",
    "encoded_embedding = transformer_token + pos_embedding\n",
    "\n",
    "# Print the positional encodings and final encoded embedding\n",
    "print(\"\\nPositional Encoding:\")\n",
    "print(pos_embedding)\n",
    "print(\"\\nFinal Encoded Embedding:\")\n",
    "print(encoded_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36ffd0",
   "metadata": {},
   "source": [
    "Great! Now that we have a sense on how Positional Embeddings are created and added to Input Embeddings, let's create a PositionalEncoding class that allows us to input embeddings and create positional embeddings. This class will calculate the positional encodings and add them to the provided embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb4badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embeddings:\n",
      "tensor([[ 0.0428, -0.5897, -1.4232, -0.2149],\n",
      "        [ 0.0338, -0.7020, -1.5582,  0.4966],\n",
      "        [ 0.5453, -0.2923,  0.2946, -1.0503]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional Encoding:\n",
      "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "         [ 0.9093, -0.4161,  0.0200,  0.9998]]])\n",
      "\n",
      "Final Encoded Embeddings:\n",
      "tensor([[[ 0.0428,  0.4103, -1.4232,  0.7851],\n",
      "         [ 0.8752, -0.1617, -1.5482,  1.4966],\n",
      "         [ 1.4546, -0.7085,  0.3146, -0.0505]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding module to add positional information to the embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=10):\n",
    "        \"\"\"\n",
    "        Initialize the positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            d_model (int): The dimension of the model.\n",
    "            max_len (int): The maximum length of the sequences.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a positional encoding matrix as per the formula\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        den = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(pos * den)\n",
    "        self.encoding[:, 1::2] = torch.cos(pos * den)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, seq_len, d_model).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The input tensor with added positional encoding.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.encoding[:, :seq_len, :].to(x.device)\n",
    "\n",
    "# Define parameters\n",
    "emb_size = 4\n",
    "seq_len = 3  # Length of the sequence: \"Transformers are awesome\"\n",
    "\n",
    "# Vocabulary and token indices\n",
    "vocab = {'Transformers': 0, 'are': 1, 'awesome': 2}\n",
    "token_indices = [vocab['Transformers'], vocab['are'], vocab['awesome']]\n",
    "\n",
    "VOCAB_SIZE = len(vocab)  # Size of the vocabulary\n",
    "EMBEDDING_DIM = 4  # Dimension of the embedding vector\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "# Convert token_indices to a tensor\n",
    "token_indices_tensor = torch.tensor(token_indices, dtype=torch.long)\n",
    "\n",
    "# Generate embeddings for the given indices\n",
    "embeddings = embedding(token_indices_tensor)\n",
    "\n",
    "print(f\"\\nWord embeddings:\")\n",
    "print(embeddings)\n",
    "\n",
    "# Initialize the positional encoding module with a smaller max_len\n",
    "pos_encoding = PositionalEncoding(d_model=emb_size, max_len=10)  # Adjusted max_len\n",
    "\n",
    "# Add positional encodings to the token embeddings\n",
    "encoded_embeddings = pos_encoding(embeddings.unsqueeze(0))\n",
    "\n",
    "# Print the positional encodings and final encoded embeddings\n",
    "print(\"\\nPositional Encoding:\")\n",
    "print(pos_encoding.encoding[:, :seq_len, :])\n",
    "print(\"\\nFinal Encoded Embeddings:\")\n",
    "print(encoded_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a691f1d",
   "metadata": {},
   "source": [
    "The output above gives us the ability to run a full sequence through a PositionalEncoding class. \n",
    "\n",
    "1. We can see the Input embeddings we created using the input sequence \"Transformers are awesome\".\n",
    "\n",
    "2. We used sine and cosine functions to generate positional encodings based on the position of each word in the sequence.\n",
    "\n",
    "3. he positional encodings were added to the input embeddings, resulting in the final encoded embeddings. This process adds positional information to the embeddings, allowing the model to understand the order of words in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf042b",
   "metadata": {},
   "source": [
    "Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918190cd",
   "metadata": {},
   "source": [
    "- Attention mechanisms emply the query, key, and value matrices.\n",
    "- The query vector should align with the same row across keyand value matrices.\n",
    "- YOu can apply attention mechanisms to word embeddings. This process helps capture contextual relationships between words.\n",
    "- You can refine the attensino formula by incorperating the softmax function on the output of the dot product between the query vector and the key.\n",
    "- For employing attension to sequences, you can conslidate all the query vectors into a single matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4ca9c",
   "metadata": {},
   "source": [
    "Self-Attention Mechinisms\n",
    "\n",
    "This self-attension mechanism is the heart of the language transformer.  Each word in the sequence attends every other word in parallel to generate embeddings.\n",
    "\n",
    "This predicts the next word in the sentence. \n",
    "\n",
    "For example, in the table the sequ3nce of words is presented on the left side and the prediction of words in on the right:\n",
    "\n",
    "| Input sequence (wt-2,wt-1)|  Predicted word  |\n",
    "|---------------------------|------------------|\n",
    "|         Not like          |       Hate       |\n",
    "|         Not hate          |       Like       |\n",
    "|         Do like           |       Like       |\n",
    "|         Do hate           |       Hate       |\n",
    "\n",
    "When you insert the sequence 'Not Like' the language modeling will predict 'Hate'.  Similarily, the input word 'Do like' would predict the word like. When the context of the word changes, the meaning of the proceeding words also changes.\n",
    "\n",
    "The words transform into matrices of the embedding sequence where each embedded word represents a column vector within the sequence matrix.\n",
    "\n",
    "![Image](/Users/williamzebrowski/transform_llm/images/atten-mech-wembed.png)\n",
    "\n",
    "For example, X, represents the matrix and the subscript 'Not, Like' represents the word sequence with each column corresponding to a word embedding.\n",
    "\n",
    "![Image](/Users/williamzebrowski/transform_llm/images/atten-mech-matrix-subs.png)\n",
    "\n",
    "This means, the Matrix X, 'Not Like' contains the embedding for 'Not' and 'Like'.\n",
    "\n",
    "Therefor, each matrix is converted into a matrix representation and represented as a sample sequence in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5fe4f",
   "metadata": {},
   "source": [
    "### Query projections with bias\n",
    "\n",
    "![Image](/Users/williamzebrowski/transform_llm/images/query-learnable-params.png)\n",
    "\n",
    "\n",
    "Self-attention mechanisms and language modeling involve 3 key components, `query Q`, `key K` and `value V`.\n",
    "\n",
    "This process begins with input embeddings combined with learnable parameters.  You can derive a query matrix by multiplying the input sequence matrix by learnable parameters known as query projections `weights and biases`.\n",
    "\n",
    "Here, a row vector of 1's with the same length as the number of tokens is also used. \n",
    "\n",
    "### Key projections with bias\n",
    "\n",
    "![Image](/Users/williamzebrowski/transform_llm/images/key-learnable-params.png)\n",
    "\n",
    "Similarily, you can derive the key matrix by multilying the input sequence matrix with a different set of learnable parameters, key projection `weights and biases`\n",
    "\n",
    "\n",
    "### Key projections with bias\n",
    "\n",
    "![Image](/Users/williamzebrowski/transform_llm/images/value-learnable-params.png)\n",
    "\n",
    "You can apply the same process to obtain the value matrix, where the input is again multiplied by a set of learnable parameters know as value projection `weights and biases`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed92cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Token Embeddings:\n",
      "tensor([[-2.1941e-01, -1.9092e-04,  7.6247e-01, -9.9991e-01],\n",
      "        [ 2.3665e+00, -7.6504e-01,  7.8260e-01, -9.7778e-01],\n",
      "        [-3.4908e-01, -6.9195e-02, -3.0242e-01, -9.7220e-02]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assume emb_size = 4 for simplicity\n",
    "emb_size = 4\n",
    "# Number of words in the phrase\n",
    "num_words = 3\n",
    "\n",
    "# Create dummy embeddings for the phrase \"Transformers are awesome\"\n",
    "token_embeddings = torch.randn(num_words, emb_size)\n",
    "print(\"Initial Token Embeddings:\")\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfaf5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, dropout, maxlen=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply the positional encodings to the input token embeddings\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac40ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49f2efa2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1aff3ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "354c6aec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d1c7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchtext torch matplotlib pandas numpy tqdm torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259233aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchdata in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: filelock in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from torchdata) (2.2.2)\n",
      "Requirement already satisfied: requests in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from torchdata) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from requests->torchdata) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from requests->torchdata) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from requests->torchdata) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/transforms.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/functional.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import YahooAnswers\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "229f4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamzebrowski/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Package `portalocker` is required to be installed to use this datapipe.Please use `pip install 'portalocker>=2.0.0'` or`conda install -c conda-forge 'portalocker>=2/0.0'`to install the package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:38\u001b[0m, in \u001b[0;36m_assert_portalocker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mportalocker\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'portalocker'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     11\u001b[0m data_set_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# We'll be using the YahooAnswers Dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Note that for torchtext these datasets are NOT Pytorch dataset classes \"YahooAnswers\" is a function that\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# returns a Pytorch DataPipe!\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Un-comment to triger the DataPipe to download the data vvv\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mYahooAnswers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_set_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset_train))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Side-Note I've noticed that the WikiText dataset is no longer able to be downloaded :(\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/data/datasets_utils.py:193\u001b[0m, in \u001b[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001b[0;34m(root, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(new_root):\n\u001b[1;32m    192\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(new_root, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/data/datasets_utils.py:155\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[0;34m(root, split, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m _check_default_set(split, splits, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_datasets(\u001b[38;5;28mtuple\u001b[39m(result), split)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchtext/datasets/yahooanswers.py:80\u001b[0m, in \u001b[0;36mYahooAnswers\u001b[0;34m(root, split)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileOpener, GDriveReader, HttpReader, IterableWrapper  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     78\u001b[0m url_dp \u001b[38;5;241m=\u001b[39m IterableWrapper([URL])\n\u001b[0;32m---> 80\u001b[0m cache_compressed_dp \u001b[38;5;241m=\u001b[39m \u001b[43murl_dp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_disk_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_filepath_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m_filepath_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMD5\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m cache_compressed_dp \u001b[38;5;241m=\u001b[39m GDriveReader(cache_compressed_dp)\u001b[38;5;241m.\u001b[39mend_caching(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, same_filepath_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m cache_decompressed_dp \u001b[38;5;241m=\u001b[39m cache_compressed_dp\u001b[38;5;241m.\u001b[39mon_disk_cache(filepath_fn\u001b[38;5;241m=\u001b[39mpartial(_extracted_filepath_fn, root, split))\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torch/utils/data/datapipes/datapipe.py:138\u001b[0m, in \u001b[0;36mIterDataPipe.register_datapipe_as_function.<locals>.class_function\u001b[0;34m(cls, enable_df_api_tracing, source_dp, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclass_function\u001b[39m(\u001b[38;5;28mcls\u001b[39m, enable_df_api_tracing, source_dp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 138\u001b[0m     result_pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource_dp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_pipe, IterDataPipe):\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m enable_df_api_tracing \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source_dp, DFIterDataPipe):\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:208\u001b[0m, in \u001b[0;36mOnDiskCacheHolderIterDataPipe.__init__\u001b[0;34m(self, source_datapipe, filepath_fn, hash_dict, hash_type, extra_check_fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    202\u001b[0m     source_datapipe: IterDataPipe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     extra_check_fn: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ):\n\u001b[0;32m--> 208\u001b[0m     \u001b[43m_assert_portalocker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_datapipe \u001b[38;5;241m=\u001b[39m source_datapipe\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filepath_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/transform_llm/.venv/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:47\u001b[0m, in \u001b[0;36m_assert_portalocker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage `portalocker` is required to be installed to use this datapipe.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportalocker>=2.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`conda install -c conda-forge \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportalocker>=2/0.0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto install the package\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Package `portalocker` is required to be installed to use this datapipe.Please use `pip install 'portalocker>=2.0.0'` or`conda install -c conda-forge 'portalocker>=2/0.0'`to install the package"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters\n",
    "learning_rate = 1e-4\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "max_len_q = 32\n",
    "max_len_a = 64\n",
    "\n",
    "data_set_root = \"../datasets\"\n",
    "\n",
    "# We'll be using the YahooAnswers Dataset\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"YahooAnswers\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "\n",
    "# Depending on the dataset sometimes the dataset doesn't download and gives an error\n",
    "# and you'll have to download and extract manually \n",
    "# \"The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status\"\n",
    "\n",
    "# Un-comment to triger the DataPipe to download the data vvv\n",
    "dataset_train = YahooAnswers(root=data_set_root, split=\"train\")\n",
    "data = next(iter(dataset_train))\n",
    "\n",
    "# Side-Note I've noticed that the WikiText dataset is no longer able to be downloaded :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## \"Train\" a Sentence Piece Tokenizer with the train data capping the vocab size to 20000 tokens\n",
    "# from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "# with open(os.path.join(data_set_root, \"datasets/YahooAnswers/train.csv\")) as f:\n",
    "#     with open(os.path.join(data_set_root, \"datasets/YahooAnswers/data.txt\"), \"w\") as f2:\n",
    "#         for i, line in enumerate(f):\n",
    "#             text_only = \"\".join(line.split(\",\")[1:])\n",
    "#             filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "#             f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "\n",
    "# generate_sp_model(os.path.join(data_set_root, \"datasets/YahooAnswers/data.txt\"), \n",
    "#                   vocab_size=20000, model_prefix='spm_user_ya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooQA(Dataset):\n",
    "    def __init__(self, num_datapoints, test_train=\"train\"):\n",
    "        self.df = pd.read_csv(os.path.join(data_set_root, \"datasets/YahooAnswers/\" + test_train + \".csv\"),\n",
    "                              names=[\"Class\", \"Q_Title\", \"Q_Content\", \"A\"])\n",
    "        \n",
    "        self.df.fillna('', inplace=True)\n",
    "        self.df['Q'] = self.df['Q_Title'] + ': ' + self.df['Q_Content']\n",
    "        self.df.drop(['Q_Title', 'Q_Content'], axis=1, inplace=True)\n",
    "        self.df['Q'] = self.df['Q'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "        self.df['A'] = self.df['A'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question_text = self.df.loc[index][\"Q\"].lower()\n",
    "        answer_text = self.df.loc[index][\"A\"].lower()\n",
    "\n",
    "        return question_text, answer_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = YahooQA(num_datapoints=data_set_root, test_train=\"train\")\n",
    "dataset_test = YahooQA(num_datapoints=data_set_root, test_train=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = load_sp_model(\"spm_user_ya.model\")\n",
    "tokenizer = sentencepiece_tokenizer(sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "            \n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"spm_user_ya.vocab\"), \n",
    "                                  specials= ['<pad>', '<soq>', '<eoq>', '<soa>', '<eoa>', '<unk>'], # special case tokens\n",
    "                                  special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_transform = T.SentencePieceTokenizer(\"spm_user_ya.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72346ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_user_ya.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len_q),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n",
    "\n",
    "a_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_user_ya.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(3, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len_a),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(4, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal positional embeds\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    \n",
    "# Attention block with self-attention with/without causal masking\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.masking = masking\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True)\n",
    "                \n",
    "    def forward(self, x_in, kv_in):\n",
    "        if self.masking:\n",
    "            bs, l, h = x_in.shape\n",
    "            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n",
    "        else:\n",
    "            mask = None\n",
    "            \n",
    "        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask)[0]\n",
    "\n",
    "    \n",
    "# Transformer block with self-attention with/without causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, decoder=False, masking=True):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n",
    "        \n",
    "        if self.decoder:\n",
    "            self.norm2 = nn.LayerNorm(hidden_size)\n",
    "            self.attn2 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=False)\n",
    "        \n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size * 4, hidden_size))\n",
    "                \n",
    "    def forward(self, x, kv_cross=None):\n",
    "        x = self.attn1(x, x) + x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        if self.decoder:\n",
    "            x = self.attn2(x, kv_cross) + x\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        x = self.mlp(x) + x\n",
    "        return self.norm_mlp(x)\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, decoder=False, masking=False) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "    def forward(self, input_seq):        \n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            output = block(embs)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, decoder=True) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "        \n",
    "    def forward(self, input_seq, encoder_output):        \n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            output = block(embs, kv_cross=encoder_output)\n",
    "        \n",
    "        return self.fc_out(output)\n",
    "\n",
    "    \n",
    "# \"Encoder-Decoder\" Style Transformer with self-attention\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.encoder = Encoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[0], num_heads=num_heads)\n",
    "        \n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "        \n",
    "    def forward(self, input_seq, target_seq):        \n",
    "        encoded_seq = self.encoder(input_seq)\n",
    "        decoded_seq = self.decoder(target_seq, encoded_seq)\n",
    "\n",
    "        return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "\n",
    "num_layers = (3, 6)\n",
    "num_heads = 16\n",
    "\n",
    "# Create model\n",
    "tf_generator = EncoderDecoder(num_emb=len(vocab), num_layers=num_layers, \n",
    "                              hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "# td = TokenDrop(prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0443831",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n",
    "    tf_generator.train()\n",
    "    steps = 0\n",
    "    for q_text, a_text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        q_text_tokens = q_tranform(list(q_text)).to(device)\n",
    "        a_text_tokens = a_tranform(list(a_text)).to(device)\n",
    "        a_input_text = a_text_tokens[:, 0:-1]\n",
    "        a_output_text = a_text_tokens[:, 1:]\n",
    "        \n",
    "        bs = q_text_tokens.shape[0]\n",
    "\n",
    "        pred = tf_generator(q_text_tokens, a_input_text)\n",
    "\n",
    "        loss = loss_fn(pred.transpose(1, 2), a_output_text)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss_logger.append(loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[1000:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd144a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 512\n",
    "data = np.convolve(np.array(training_loss_logger), np.ones(window_size)/window_size, mode=\"valid\")\n",
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(data[10000:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_text, a_text = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeeb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_prompt = [\"what is that largest ocean in the world?: \"]\n",
    "init_prompt = [q_text[0]]\n",
    "\n",
    "input_tokens = q_tranform(init_prompt).to(device)\n",
    "\n",
    "# Add Start-Of-Answer token to prompt the network to start generating the answer!\n",
    "# input_tokens = torch.cat((input_tokens, 3 * torch.ones(1, 1, device=device).long()), 1)\n",
    "soa_token = 3 * torch.ones(1, 1).long()\n",
    "print(input_tokens)\n",
    "print(vocab.lookup_tokens(input_tokens[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tokens = [soa_token]\n",
    "tf_generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_seq = tf_generator.encoder(input_tokens.to(device))\n",
    "\n",
    "    for i in range(100):\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        data_pred = tf_generator.decoder(input_tokens.to(device), encoded_seq)\n",
    "#         We can take the token with the highest prob\n",
    "#         input_tokens = data_pred[:, -1].argmax().reshape(1, 1)\n",
    "        \n",
    "        # Or sample from the distribution of probs!\n",
    "        dist = Categorical(logits=data_pred[:, -1]/temp)\n",
    "        next_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        \n",
    "        if next_tokens.item() == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text = \"\".join(vocab.lookup_tokens(torch.cat(log_tokens, 1)[0].numpy()))\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765963d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text.replace(\"\", \" \").replace(\"<unk>\", \"\").replace(\"<eoa>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F.softmax(data_pred[:, -1]/temp, -1).cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc3b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0dea41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
