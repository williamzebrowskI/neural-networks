{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanism\n",
    "\n",
    "The self-attention mechanism allows the model to weigh the importance of different words in a sequence. It helps the model focus on relevant words while encoding a particular word. This is a critical part of the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prior module, We left off with the position encoding embedding. To continue this process, we are pushing the positional embedding into the next compontent of the transformer, the attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoded Embeddings Shape: torch.Size([1, 10, 10])\n",
      "tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
      "           0.4198,  2.7427,  0.2844],\n",
      "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
      "           0.4085,  1.3560, -1.3176],\n",
      "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
      "          -0.6907, -0.4066,  1.2899],\n",
      "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
      "           0.9153, -1.1734,  1.1989],\n",
      "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
      "           0.6083,  0.2095,  1.5739],\n",
      "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
      "           0.2480, -0.2364,  0.8417],\n",
      "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
      "           1.8013,  0.7900, -0.9430],\n",
      "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
      "           1.8325,  0.3009,  1.0083],\n",
      "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
      "           0.6484,  0.9932,  0.4030],\n",
      "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
      "          -0.8118,  0.3455,  0.9474]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Positional Encoded Embeddings from the previous notebook\n",
    "pos_encoded_embeddings = torch.tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
    "           0.4198,  2.7427,  0.2844],\n",
    "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
    "           0.4085,  1.3560, -1.3176],\n",
    "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
    "          -0.6907, -0.4066,  1.2899],\n",
    "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
    "           0.9153, -1.1734,  1.1989],\n",
    "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
    "           0.6083,  0.2095,  1.5739],\n",
    "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
    "           0.2480, -0.2364,  0.8417],\n",
    "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
    "           1.8013,  0.7900, -0.9430],\n",
    "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
    "           1.8325,  0.3009,  1.0083],\n",
    "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
    "           0.6484,  0.9932,  0.4030],\n",
    "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
    "          -0.8118,  0.3455,  0.9474]]], dtype=torch.float)\n",
    "\n",
    "print(\"Positional Encoded Embeddings Shape:\", pos_encoded_embeddings.shape) # torch.Size([1, 10, 10]) because we have 1 batch, 10 tokens, and 10 features\n",
    "print(pos_encoded_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformation to Generate Queries, Keys, and Values\n",
    "\n",
    "The positional encoded embeddings are passed through three different linear layers to generate the Query (Q), Key (K), and Value (V) matrices. These matrices are used to compute the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 1 Queries Shape: torch.Size([1, 10, 5])\n",
      "Head 1 Queries Matrix:\n",
      " tensor([[[-0.3537, -0.2582, -1.0804,  1.2453,  0.3636],\n",
      "         [ 1.4551,  0.7938,  0.6904,  1.0099, -0.1060],\n",
      "         [-0.0323,  0.4988,  0.1480, -0.2169,  0.3064],\n",
      "         [ 0.1413, -0.1496, -0.1585, -0.6643, -0.1013],\n",
      "         [-0.6394,  0.0277, -0.0857,  0.1829,  0.9370],\n",
      "         [-0.1441, -0.4683, -0.6981, -0.2103, -0.1838],\n",
      "         [ 1.2703, -0.0261, -0.1065,  0.5500, -0.2545],\n",
      "         [ 1.2149, -0.0288, -0.5788,  1.1972,  0.0184],\n",
      "         [ 1.8365,  0.6732, -0.2994,  0.8017,  0.0809],\n",
      "         [ 0.1365,  0.2335, -0.1007, -0.1777,  0.2385]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Head 1 Keys Shape: torch.Size([1, 10, 5])\n",
      "Head 1 Keys Matrix:\n",
      " tensor([[[-0.1314, -0.9953,  0.4568, -0.1121, -0.4200],\n",
      "         [ 0.4794, -1.5111,  0.5368,  0.4985, -0.7619],\n",
      "         [ 0.8266, -0.8718,  0.7948, -0.7616, -0.0178],\n",
      "         [ 0.3400,  0.2035,  0.1653,  0.4253, -0.2353],\n",
      "         [-0.3605,  0.5337,  0.9110,  0.4017, -0.0616],\n",
      "         [ 0.2438,  0.1821, -0.2392,  0.2995, -0.2978],\n",
      "         [-0.0512, -0.6848, -0.1089,  1.1595, -0.6875],\n",
      "         [-0.0811, -0.3348,  0.2683,  0.6335, -0.8099],\n",
      "         [ 0.7490, -1.7636,  0.6541,  0.1700, -0.6845],\n",
      "         [ 0.8932, -0.1813,  0.4052,  0.1491, -0.1435]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Head 1 Values Shape: torch.Size([1, 10, 5])\n",
      "Head 1 Values Matrix:\n",
      " tensor([[[-0.5610, -0.7229,  0.8684, -0.3850,  0.8063],\n",
      "         [-0.5179,  0.1602, -0.4224, -0.6570,  0.7091],\n",
      "         [ 0.4115, -0.1150, -0.4602, -1.2939,  0.0483],\n",
      "         [ 0.2999,  0.4215,  0.3906, -0.5836,  0.2094],\n",
      "         [-0.1176,  0.3039,  0.5241, -0.5797,  0.2710],\n",
      "         [-0.3561,  0.0039,  0.4255,  0.3861,  0.5537],\n",
      "         [-0.3218,  0.3137,  0.4874, -0.0342,  0.5902],\n",
      "         [-1.0614, -0.8216,  0.6169, -0.3958,  1.1842],\n",
      "         [ 0.3126, -0.4793,  0.1521, -0.7162, -0.1920],\n",
      "         [ 0.3437,  0.1501, -0.0786, -0.0169, -0.3264]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Head 2 Queries Shape: torch.Size([1, 10, 5])\n",
      "Head 2 Queries Matrix:\n",
      " tensor([[[-0.9100,  0.7017,  0.4572, -0.5847,  0.3338],\n",
      "         [-0.6057,  0.9804, -0.9266, -0.4270,  0.6359],\n",
      "         [ 0.5585,  0.8248, -0.3548, -0.0732,  0.2081],\n",
      "         [ 0.8732,  0.5865,  0.3062,  0.3832, -0.2491],\n",
      "         [-0.5185,  1.7906,  0.9138,  1.1045, -0.6032],\n",
      "         [ 0.4340, -0.3919,  1.0272, -0.5758,  0.1938],\n",
      "         [-0.1177,  0.5875, -0.3364, -0.2209,  0.3221],\n",
      "         [-0.6018,  0.1845,  1.0646,  0.1793,  0.2352],\n",
      "         [ 0.1841,  0.4896, -1.2203, -0.7992,  0.5708],\n",
      "         [ 0.7552,  0.6610, -0.0476, -0.2281, -0.3788]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Head 2 Keys Shape: torch.Size([1, 10, 5])\n",
      "Head 2 Keys Matrix:\n",
      " tensor([[[-1.1081,  0.3241, -0.2927, -0.5184,  0.2634],\n",
      "         [-1.0415,  0.0207, -0.7142,  1.0911, -1.4143],\n",
      "         [ 0.7082, -0.5716, -0.5133,  0.9135, -0.3157],\n",
      "         [ 0.5140, -0.3473,  0.5659,  0.9661,  0.8295],\n",
      "         [ 0.5157,  0.2555, -0.2413,  0.4262,  1.3096],\n",
      "         [-0.1283,  0.6442,  1.0866, -0.2394,  1.0902],\n",
      "         [-1.1647,  0.0240,  0.2744,  0.8150, -0.3911],\n",
      "         [-0.6475,  0.5999,  0.8753, -0.5158,  1.1534],\n",
      "         [-0.9901, -0.1442, -0.6205,  0.4236, -1.9053],\n",
      "         [ 0.4934,  0.1800, -0.0057,  0.1259, -0.2086]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Head 2 Values Shape: torch.Size([1, 10, 5])\n",
      "Head 2 Values Matrix:\n",
      " tensor([[[-0.3082,  0.5496,  1.1175, -0.0281,  0.4089],\n",
      "         [ 0.4945,  0.1052,  0.7894,  0.5564,  0.6073],\n",
      "         [ 0.7097,  0.7071, -0.5978,  0.3849,  0.1858],\n",
      "         [ 1.0303,  1.1451, -1.0493, -0.2159,  0.8224],\n",
      "         [ 1.0457,  0.8432,  0.0784, -0.5928,  1.3208],\n",
      "         [-0.3531,  0.8015, -0.3839, -0.4338, -0.1241],\n",
      "         [ 0.5221,  0.2990,  0.2970,  0.1912,  0.8730],\n",
      "         [ 0.3649,  0.5278,  0.3156,  0.0017, -0.3761],\n",
      "         [-0.1766,  0.0015,  0.1282,  0.8850, -0.2935],\n",
      "         [-0.2765,  1.0362, -0.6627, -0.2574,  0.6065]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "d_model = pos_encoded_embeddings.size(-1)\n",
    "d_k = d_model // 2  # Assuming d_k = d_model / num_heads for simplicity\n",
    "num_heads = 2\n",
    "\n",
    "# Create nn.Linear layers for Q, K, V for each head\n",
    "W_q = nn.ModuleList([nn.Linear(d_model, d_k) for _ in range(num_heads)])\n",
    "W_k = nn.ModuleList([nn.Linear(d_model, d_k) for _ in range(num_heads)])\n",
    "W_v = nn.ModuleList([nn.Linear(d_model, d_k) for _ in range(num_heads)])\n",
    "\n",
    "# Apply the linear transformations to generate Q, K, V for each head\n",
    "queries = [W_q[i](pos_encoded_embeddings) for i in range(num_heads)]\n",
    "keys = [W_k[i](pos_encoded_embeddings) for i in range(num_heads)]\n",
    "values = [W_v[i](pos_encoded_embeddings) for i in range(num_heads)]\n",
    "\n",
    "# Print the shapes and matrices of each head before concatenation\n",
    "for i in range(num_heads):\n",
    "    print(f\"Head {i+1} Queries Shape:\", queries[i].shape)\n",
    "    print(f\"Head {i+1} Queries Matrix:\\n\", queries[i])\n",
    "    print(f\"Head {i+1} Keys Shape:\", keys[i].shape)\n",
    "    print(f\"Head {i+1} Keys Matrix:\\n\", keys[i])\n",
    "    print(f\"Head {i+1} Values Shape:\", values[i].shape)\n",
    "    print(f\"Head {i+1} Values Matrix:\\n\", values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "To compute the attention scores, we take the dot product of the Query and Key matrices. The result is then scaled by the square root of the dimension of the keys (\\(d_k\\)) to prevent the values from becoming too large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squre root of the dim of keys: 2.23606797749979\n",
      "Head 1 Scores Shape: torch.Size([1, 10, 10])\n",
      "Head 1 Scores Matrix:\n",
      " tensor([[[-0.2157, -0.0069, -0.8411,  0.0414, -0.2311,  0.1743,  0.6738,\n",
      "           0.1430, -0.2474, -0.2564],\n",
      "         [-0.3285,  0.2026,  0.1307,  0.5478,  0.4205,  0.2989,  0.2462,\n",
      "           0.2357,  0.1726,  0.7161],\n",
      "         [-0.2366, -0.4612, -0.0824, -0.0221,  0.1372, -0.0486, -0.3659,\n",
      "          -0.2282, -0.4712, -0.0607],\n",
      "         [ 0.0782, -0.0202,  0.2813, -0.1195, -0.2396, -0.0553, -0.2630,\n",
      "          -0.1533,  0.0995,  0.0021],\n",
      "         [-0.1774, -0.4549, -0.3473, -0.1649,  0.0818, -0.1586, -0.1829,\n",
      "          -0.2788, -0.5340, -0.3211],\n",
      "         [ 0.1193,  0.1337, -0.0457, -0.1368, -0.4056,  0.0171,  0.1282,\n",
      "          -0.0014,  0.1571, -0.1483],\n",
      "         [-0.0645,  0.4738,  0.2566,  0.3143, -0.1486,  0.2553,  0.3476,\n",
      "           0.1931,  0.5347,  0.5432],\n",
      "         [-0.2403,  0.4017, -0.1533,  0.3651, -0.2240,  0.3499,  0.6244,\n",
      "           0.2233,  0.3458,  0.4614],\n",
      "         [-0.5241,  0.0181,  0.0363,  0.4624, -0.1156,  0.3837,  0.1572,\n",
      "          -0.0055,  0.0328,  0.6730],\n",
      "         [-0.1684, -0.2736, -0.0178, -0.0243, -0.0458, -0.0109, -0.2352,\n",
      "          -0.1887, -0.2544, -0.0098]]], grad_fn=<DivBackward0>)\n",
      "Head 2 Scores Shape: torch.Size([1, 10, 10])\n",
      "Head 2 Scores Matrix:\n",
      " tensor([[[ 0.6677, -0.2121, -0.8585, -0.3312, -0.0950,  0.7019,  0.2662,\n",
      "           0.9378, -0.1644, -0.2095],\n",
      "         [ 0.7375, -0.0234, -0.4940, -0.4747,  0.3633,  0.2226, -0.0545,\n",
      "           0.5022, -0.1606, -0.1357],\n",
      "         [-0.0693, -0.3065, -0.0118, -0.0440,  0.3692,  0.1424, -0.3887,\n",
      "           0.0449, -0.3932,  0.1670],\n",
      "         [-0.5060, -0.1545,  0.2481,  0.2603,  0.1625,  0.1052, -0.2277,\n",
      "          -0.1925, -0.2246,  0.2839],\n",
      "         [ 0.0698,  0.8867, -0.2953,  0.0874, -0.1563,  0.5774,  0.9094,\n",
      "           0.4224,  0.5837,  0.1459],\n",
      "         [-0.2500, -0.9374, -0.2608,  0.2437, -0.0518,  0.5175, -0.3479,\n",
      "           0.4041, -0.7261,  0.0111],\n",
      "         [ 0.2767, -0.1438, -0.2459, -0.1794,  0.2228,  0.1932, -0.1105,\n",
      "           0.2771, -0.2087, -0.0203],\n",
      "         [ 0.1717, -0.1192, -0.4420,  0.2672, -0.0607,  0.7005,  0.4703,\n",
      "           0.7204, -0.2072, -0.1325],\n",
      "         [ 0.3920, -0.4425, -0.1938, -0.4761,  0.4120, -0.0987, -0.6315,\n",
      "           0.0792, -0.4123, -0.0151],\n",
      "         [-0.2640, -0.2021,  0.0414, -0.1802, -0.0105, -0.0363, -0.4089,\n",
      "          -0.2028, -0.0843,  0.2425]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the dot product of queries and keys, then scale\n",
    "print(f\"squre root of the dim of keys: {math.sqrt(d_k)}\")\n",
    "\n",
    "# Compute the dot product of queries and keys, then scale for each head\n",
    "scores = [torch.matmul(queries[i], keys[i].transpose(-2, -1)) / math.sqrt(d_k) for i in range(num_heads)]\n",
    "\n",
    "# Print the scores of each head\n",
    "for i in range(num_heads):\n",
    "    print(f\"Head {i+1} Scores Shape:\", scores[i].shape)\n",
    "    print(f\"Head {i+1} Scores Matrix:\\n\", scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax to Get Attention Weights\n",
    "\n",
    "The scaled dot-product scores are then passed through a softmax function to convert them into probabilities. These probabilities represent the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 1 Attention Weights Shape: torch.Size([1, 10, 10])\n",
      "Head 1 Attention Weights Matrix:\n",
      " tensor([[[0.0812, 0.1000, 0.0434, 0.1050, 0.0800, 0.1199, 0.1976, 0.1162,\n",
      "          0.0787, 0.0780],\n",
      "         [0.0535, 0.0909, 0.0846, 0.1284, 0.1131, 0.1001, 0.0950, 0.0940,\n",
      "          0.0883, 0.1520],\n",
      "         [0.0932, 0.0744, 0.1087, 0.1154, 0.1354, 0.1124, 0.0819, 0.0939,\n",
      "          0.0737, 0.1111],\n",
      "         [0.1110, 0.1006, 0.1360, 0.0911, 0.0808, 0.0971, 0.0789, 0.0881,\n",
      "          0.1134, 0.1029],\n",
      "         [0.1065, 0.0807, 0.0898, 0.1078, 0.1380, 0.1085, 0.1059, 0.0962,\n",
      "          0.0745, 0.0922],\n",
      "         [0.1133, 0.1149, 0.0960, 0.0877, 0.0670, 0.1023, 0.1143, 0.1004,\n",
      "          0.1176, 0.0867],\n",
      "         [0.0699, 0.1197, 0.0963, 0.1021, 0.0642, 0.0962, 0.1055, 0.0904,\n",
      "          0.1272, 0.1283],\n",
      "         [0.0609, 0.1157, 0.0664, 0.1116, 0.0619, 0.1099, 0.1446, 0.0968,\n",
      "          0.1094, 0.1228],\n",
      "         [0.0504, 0.0866, 0.0882, 0.1351, 0.0758, 0.1249, 0.0996, 0.0846,\n",
      "          0.0879, 0.1668],\n",
      "         [0.0950, 0.0855, 0.1105, 0.1098, 0.1074, 0.1112, 0.0889, 0.0931,\n",
      "          0.0872, 0.1114]]], grad_fn=<SoftmaxBackward0>)\n",
      "Head 2 Attention Weights Shape: torch.Size([1, 10, 10])\n",
      "Head 2 Attention Weights Matrix:\n",
      " tensor([[[0.1579, 0.0655, 0.0343, 0.0582, 0.0737, 0.1634, 0.1057, 0.2069,\n",
      "          0.0687, 0.0657],\n",
      "         [0.1848, 0.0864, 0.0539, 0.0550, 0.1271, 0.1105, 0.0837, 0.1461,\n",
      "          0.0753, 0.0772],\n",
      "         [0.0953, 0.0751, 0.1009, 0.0977, 0.1477, 0.1177, 0.0692, 0.1068,\n",
      "          0.0689, 0.1207],\n",
      "         [0.0598, 0.0850, 0.1272, 0.1288, 0.1168, 0.1103, 0.0790, 0.0819,\n",
      "          0.0793, 0.1319],\n",
      "         [0.0718, 0.1626, 0.0499, 0.0731, 0.0573, 0.1193, 0.1663, 0.1022,\n",
      "          0.1201, 0.0775],\n",
      "         [0.0816, 0.0410, 0.0807, 0.1337, 0.0995, 0.1758, 0.0740, 0.1570,\n",
      "          0.0507, 0.1060],\n",
      "         [0.1284, 0.0843, 0.0761, 0.0814, 0.1217, 0.1181, 0.0872, 0.1284,\n",
      "          0.0790, 0.0954],\n",
      "         [0.0963, 0.0720, 0.0522, 0.1060, 0.0764, 0.1635, 0.1299, 0.1668,\n",
      "          0.0660, 0.0711],\n",
      "         [0.1601, 0.0695, 0.0891, 0.0672, 0.1633, 0.0980, 0.0575, 0.1171,\n",
      "          0.0716, 0.1066],\n",
      "         [0.0845, 0.0899, 0.1147, 0.0919, 0.1088, 0.1061, 0.0731, 0.0898,\n",
      "          0.1011, 0.1402]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to the scores to get the attention weights for each head\n",
    "attention = [torch.nn.functional.softmax(scores[i], dim=-1) for i in range(num_heads)]\n",
    "\n",
    "# Print the attention weights of each head\n",
    "for i in range(num_heads):\n",
    "    print(f\"Head {i+1} Attention Weights Shape:\", attention[i].shape)\n",
    "    print(f\"Head {i+1} Attention Weights Matrix:\\n\", attention[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Output as a Weighted Sum of the Values\n",
    "\n",
    "The final step in the attention mechanism is to compute a weighted sum of the Value matrix using the attention weights. This gives us the output of the self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 1 Output Shape: torch.Size([1, 10, 5])\n",
      "Head 1 Output:\n",
      " tensor([[[-0.2357, -0.0381,  0.3160, -0.3249,  0.4623],\n",
      "         [-0.1032, -0.0118,  0.2269, -0.3979,  0.3255],\n",
      "         [-0.1322, -0.0478,  0.2636, -0.4224,  0.3654],\n",
      "         [-0.1233, -0.1030,  0.2125, -0.4679,  0.3577],\n",
      "         [-0.1682, -0.0543,  0.2926, -0.4078,  0.3994],\n",
      "         [-0.1783, -0.1068,  0.2472, -0.4232,  0.4060],\n",
      "         [-0.1188, -0.0624,  0.1967, -0.4226,  0.3410],\n",
      "         [-0.1526, -0.0350,  0.2334, -0.3677,  0.3742],\n",
      "         [-0.0868, -0.0076,  0.2132, -0.3678,  0.3120],\n",
      "         [-0.1334, -0.0620,  0.2473, -0.4233,  0.3659]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Head 2 Output Shape: torch.Size([1, 10, 5])\n",
      "Head 2 Output:\n",
      " tensor([[[ 0.1877,  0.5866,  0.1517, -0.0174,  0.2697],\n",
      "         [ 0.2370,  0.5898,  0.1818, -0.0085,  0.3803],\n",
      "         [ 0.3225,  0.6646, -0.0476, -0.0384,  0.4374],\n",
      "         [ 0.3504,  0.6697, -0.1382,  0.0012,  0.4411],\n",
      "         [ 0.2682,  0.5036,  0.1064,  0.1244,  0.3768],\n",
      "         [ 0.2898,  0.6990, -0.1169, -0.0805,  0.3478],\n",
      "         [ 0.2775,  0.6165,  0.0567, -0.0061,  0.3933],\n",
      "         [ 0.2716,  0.6148,  0.0179, -0.0164,  0.3350],\n",
      "         [ 0.2843,  0.6410,  0.0778, -0.0382,  0.4328],\n",
      "         [ 0.2851,  0.6336, -0.0617,  0.0289,  0.4019]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the output as a weighted sum of the values for each head\n",
    "outputs = [torch.matmul(attention[i], values[i]) for i in range(num_heads)]\n",
    "\n",
    "# Print the outputs of each head before concatenation\n",
    "for i in range(num_heads):\n",
    "    print(f\"Head {i+1} Output Shape:\", outputs[i].shape)\n",
    "    print(f\"Head {i+1} Output:\\n\", outputs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. **Weighted Sum of Values to Get the Output**\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "After computing the attention weights, the next step is to calculate the final output by taking a weighted sum of the value vectors. This is done through matrix multiplication of the attention weights matrix with the value matrix.\n",
    "\n",
    "Given:\n",
    "- **Attention Weights** matrix $\\mathbf{A}$ of shape $(\\text{number of tokens}, \\text{number of tokens})$.\n",
    "- **Values** matrix $\\mathbf{V}$ of shape $(\\text{number of tokens}, d_{\\text{model}})$.\n",
    "\n",
    "The output is computed as:\n",
    "$$\n",
    "\\mathbf{O} = \\mathbf{A} \\times \\mathbf{V}\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbf{O}$ is the output matrix of shape $(\\text{number of tokens}, d_{\\text{model}})$.\n",
    "\n",
    "#### Example Code with Matrix Multiplication\n",
    "\n",
    "```python\n",
    "# Let's assume 3 tokens and a model dimension of 4 for simplicity\n",
    "values = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "], dtype=torch.float)\n",
    "\n",
    "attention_weights = torch.tensor([\n",
    "    [0.2, 0.3, 0.5],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.4, 0.1, 0.5]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Perform the matrix multiplication\n",
    "output = torch.matmul(attention_weights, values)\n",
    "\n",
    "print(\"Attention Weights Matrix (A):\\n\", attention_weights)\n",
    "print(\"Values Matrix (V):\\n\", values)\n",
    "print(\"Output Matrix (O = A * V):\\n\", output)\n",
    "```\n",
    "\n",
    "#### Expected Output\n",
    "\n",
    "Given the example matrices:\n",
    "\n",
    "- **Attention Weights (A):**\n",
    "  $$\n",
    "  \\mathbf{A} =\n",
    "  \\begin{bmatrix}\n",
    "  0.2 & 0.3 & 0.5 \\\\\n",
    "  0.1 & 0.8 & 0.1 \\\\\n",
    "  0.4 & 0.1 & 0.5\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Values (V):**\n",
    "  $$\n",
    "  \\mathbf{V} =\n",
    "  \\begin{bmatrix}\n",
    "  0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
    "  0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
    "  0.9 & 1.0 & 1.1 & 1.2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "The output matrix $ \\mathbf{O} $ will be:\n",
    "\n",
    "- **Output (O):**\n",
    "  $$\n",
    "  \\mathbf{O} =\n",
    "  \\begin{bmatrix}\n",
    "  (0.2 \\times 0.1 + 0.3 \\times 0.5 + 0.5 \\times 0.9) & (0.2 \\times 0.2 + 0.3 \\times 0.6 + 0.5 \\times 1.0) & (0.2 \\times 0.3 + 0.3 \\times 0.7 + 0.5 \\times 1.1) & (0.2 \\times 0.4 + 0.3 \\times 0.8 + 0.5 \\times 1.2) \\\\\n",
    "  (0.1 \\times 0.1 + 0.8 \\times 0.5 + 0.1 \\times 0.9) & (0.1 \\times 0.2 + 0.8 \\times 0.6 + 0.1 \\times 1.0) & (0.1 \\times 0.3 + 0.8 \\times 0.7 + 0.1 \\times 1.1) & (0.1 \\times 0.4 + 0.8 \\times 0.8 + 0.1 \\times 1.2) \\\\\n",
    "  (0.4 \\times 0.1 + 0.1 \\times 0.5 + 0.5 \\times 0.9) & (0.4 \\times 0.2 + 0.1 \\times 0.6 + 0.5 \\times 1.0) & (0.4 \\times 0.3 + 0.1 \\times 0.7 + 0.5 \\times 1.1) & (0.4 \\times 0.4 + 0.1 \\times 0.8 + 0.5 \\times 1.2)\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "Evaluating the above gives:\n",
    "\n",
    "$$\n",
    "\\mathbf{O} =\n",
    "\\begin{bmatrix}\n",
    "0.62 & 0.74 & 0.86 & 0.98 \\\\\n",
    "0.42 & 0.54 & 0.66 & 0.78 \\\\\n",
    "0.64 & 0.76 & 0.88 & 1.00\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, the output matrix $ \\mathbf{O} $ will be:\n",
    "\n",
    "```\n",
    "tensor([[0.62, 0.74, 0.86, 0.98],\n",
    "        [0.42, 0.54, 0.66, 0.78],\n",
    "        [0.64, 0.76, 0.88, 1.00]])\n",
    "```\n",
    "\n",
    "This demonstrates how each element in the output matrix is a weighted sum of the corresponding value matrix, with weights given by the attention scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Heads and Final Linear Transformation\n",
    "\n",
    "In a multi-head attention mechanism, multiple sets of Q, K, V matrices are computed and processed in parallel. These are then concatenated and passed through a final linear transformation to project them back to the original embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Output Shape: torch.Size([1, 10, 10])\n",
      "Concatenated Output:\n",
      " tensor([[[-0.2357, -0.0381,  0.3160, -0.3249,  0.4623,  0.1877,  0.5866,\n",
      "           0.1517, -0.0174,  0.2697],\n",
      "         [-0.1032, -0.0118,  0.2269, -0.3979,  0.3255,  0.2370,  0.5898,\n",
      "           0.1818, -0.0085,  0.3803],\n",
      "         [-0.1322, -0.0478,  0.2636, -0.4224,  0.3654,  0.3225,  0.6646,\n",
      "          -0.0476, -0.0384,  0.4374],\n",
      "         [-0.1233, -0.1030,  0.2125, -0.4679,  0.3577,  0.3504,  0.6697,\n",
      "          -0.1382,  0.0012,  0.4411],\n",
      "         [-0.1682, -0.0543,  0.2926, -0.4078,  0.3994,  0.2682,  0.5036,\n",
      "           0.1064,  0.1244,  0.3768],\n",
      "         [-0.1783, -0.1068,  0.2472, -0.4232,  0.4060,  0.2898,  0.6990,\n",
      "          -0.1169, -0.0805,  0.3478],\n",
      "         [-0.1188, -0.0624,  0.1967, -0.4226,  0.3410,  0.2775,  0.6165,\n",
      "           0.0567, -0.0061,  0.3933],\n",
      "         [-0.1526, -0.0350,  0.2334, -0.3677,  0.3742,  0.2716,  0.6148,\n",
      "           0.0179, -0.0164,  0.3350],\n",
      "         [-0.0868, -0.0076,  0.2132, -0.3678,  0.3120,  0.2843,  0.6410,\n",
      "           0.0778, -0.0382,  0.4328],\n",
      "         [-0.1334, -0.0620,  0.2473, -0.4233,  0.3659,  0.2851,  0.6336,\n",
      "          -0.0617,  0.0289,  0.4019]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the outputs of the heads\n",
    "concatenated_output = torch.cat(outputs, dim=-1)\n",
    "\n",
    "print(\"Concatenated Output Shape:\", concatenated_output.shape)\n",
    "print(\"Concatenated Output:\\n\", concatenated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Linear Transformation\n",
    "\n",
    "Project the concatenated output back to the original embedding size and print the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output Shape: torch.Size([1, 10, 10])\n",
      "Final Output:\n",
      " tensor([[[ 0.2094, -0.0633,  0.1740, -0.0485,  0.0294, -0.0105,  0.1539,\n",
      "          -0.1829,  0.3235, -0.3173],\n",
      "         [ 0.2478, -0.1173,  0.2454, -0.1086,  0.0644, -0.0207,  0.0795,\n",
      "          -0.1813,  0.3423, -0.4046],\n",
      "         [ 0.2756, -0.1733,  0.2607, -0.1177,  0.1134, -0.0120,  0.0767,\n",
      "          -0.2021,  0.3376, -0.3698],\n",
      "         [ 0.2832, -0.2060,  0.2393, -0.1575,  0.1465, -0.0202,  0.0634,\n",
      "          -0.2309,  0.2945, -0.3711],\n",
      "         [ 0.2116, -0.1373,  0.2108, -0.1043,  0.0134, -0.0571,  0.0871,\n",
      "          -0.1843,  0.2657, -0.3838],\n",
      "         [ 0.2753, -0.1495,  0.2082, -0.1235,  0.1485,  0.0069,  0.1184,\n",
      "          -0.2319,  0.3103, -0.3211],\n",
      "         [ 0.2634, -0.1516,  0.2280, -0.1352,  0.1038, -0.0176,  0.0745,\n",
      "          -0.2084,  0.3178, -0.3861],\n",
      "         [ 0.2509, -0.1427,  0.2084, -0.1089,  0.0855, -0.0055,  0.1038,\n",
      "          -0.2008,  0.3129, -0.3503],\n",
      "         [ 0.2705, -0.1674,  0.2583, -0.1134,  0.0936, -0.0058,  0.0687,\n",
      "          -0.1851,  0.3691, -0.3995],\n",
      "         [ 0.2504, -0.1718,  0.2234, -0.1291,  0.0959, -0.0184,  0.0847,\n",
      "          -0.2070,  0.2894, -0.3611]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Final linear transformation to project back to the original embedding size\n",
    "fc_out = nn.Linear(d_model, d_model)\n",
    "final_output = fc_out(concatenated_output)\n",
    "\n",
    "print(\"Final Output Shape:\", final_output.shape)\n",
    "print(\"Final Output:\\n\", final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
