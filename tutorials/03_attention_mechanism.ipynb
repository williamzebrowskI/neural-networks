{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanism\n",
    "\n",
    "The self-attention mechanism allows the model to weigh the importance of different words in a sequence. It helps the model focus on relevant words while encoding a particular word. This is a critical part of the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prior module, We left off with the position encoding embedding. To continue this process, we are pushing the positional embedding into the next compontent of the transformer, the attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoded Embeddings Shape: torch.Size([1, 10, 10])\n",
      "tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
      "           0.4198,  2.7427,  0.2844],\n",
      "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
      "           0.4085,  1.3560, -1.3176],\n",
      "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
      "          -0.6907, -0.4066,  1.2899],\n",
      "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
      "           0.9153, -1.1734,  1.1989],\n",
      "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
      "           0.6083,  0.2095,  1.5739],\n",
      "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
      "           0.2480, -0.2364,  0.8417],\n",
      "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
      "           1.8013,  0.7900, -0.9430],\n",
      "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
      "           1.8325,  0.3009,  1.0083],\n",
      "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
      "           0.6484,  0.9932,  0.4030],\n",
      "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
      "          -0.8118,  0.3455,  0.9474]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Positional Encoded Embeddings from the previous notebook\n",
    "pos_encoded_embeddings = torch.tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
    "           0.4198,  2.7427,  0.2844],\n",
    "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
    "           0.4085,  1.3560, -1.3176],\n",
    "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
    "          -0.6907, -0.4066,  1.2899],\n",
    "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
    "           0.9153, -1.1734,  1.1989],\n",
    "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
    "           0.6083,  0.2095,  1.5739],\n",
    "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
    "           0.2480, -0.2364,  0.8417],\n",
    "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
    "           1.8013,  0.7900, -0.9430],\n",
    "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
    "           1.8325,  0.3009,  1.0083],\n",
    "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
    "           0.6484,  0.9932,  0.4030],\n",
    "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
    "          -0.8118,  0.3455,  0.9474]]], dtype=torch.float)\n",
    "\n",
    "print(\"Positional Encoded Embeddings Shape:\", pos_encoded_embeddings.shape)\n",
    "print(pos_encoded_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Size: 10\n",
      "Number of Heads: 2\n",
      "Dimension per Head: 5\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions\n",
    "embedding_size = pos_encoded_embeddings.size(2)\n",
    "# embedding_size is the size of the embeddings vector for each word\n",
    "heads = 2\n",
    "# heads is the number of attention heads, allowing the model to focus on different parts of the sentence simultaneously\n",
    "head_dim = embedding_size // heads\n",
    "# head_dim is the size of each attention head, ensuring that the embedding size is evenly divisible by the number of heads\n",
    "\n",
    "print(\"Embedding Size:\", embedding_size)\n",
    "print(\"Number of Heads:\", heads)\n",
    "print(\"Dimension per Head:\", head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layers for values, keys, and queries are defined.\n"
     ]
    }
   ],
   "source": [
    "# Define linear transformations for query, key, and value\n",
    "values_linear = nn.Linear(head_dim, head_dim, bias=False)\n",
    "keys_linear = nn.Linear(head_dim, head_dim, bias=False)\n",
    "queries_linear = nn.Linear(head_dim, head_dim, bias=False)\n",
    "\n",
    "# These linear layers project the input embeddings into different vector spaces (query, key, and value)\n",
    "# This helps in calculating attention scores and obtaining the relevant context for each word\n",
    "\n",
    "print(\"Linear layers for values, keys, and queries are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value len: 10\n",
      "key len: 10\n",
      "query len: 10\n",
      "Values Shape after Reshape: torch.Size([1, 10, 2, 5])\n",
      "Keys Shape after Reshape: torch.Size([1, 10, 2, 5])\n",
      "Queries Shape after Reshape: torch.Size([1, 10, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the input embeddings to split into heads\n",
    "N = pos_encoded_embeddings.shape[0]\n",
    "# N is the batch size, which is the number of sentences being processed at once\n",
    "value_len, key_len, query_len = pos_encoded_embeddings.shape[1], pos_encoded_embeddings.shape[1], pos_encoded_embeddings.shape[1]\n",
    "print(f\"value len: {value_len}\")\n",
    "print(f\"key len: {value_len}\")\n",
    "print(f\"query len: {value_len}\")\n",
    "# value_len, key_len, and query_len are the lengths of the input sequences\n",
    "\n",
    "values = pos_encoded_embeddings.reshape(N, value_len, heads, head_dim)\n",
    "keys = pos_encoded_embeddings.reshape(N, key_len, heads, head_dim)\n",
    "queries = pos_encoded_embeddings.reshape(N, query_len, heads, head_dim)\n",
    "# The input embeddings are reshaped to separate the heads for multi-head attention\n",
    "\n",
    "print(\"Values Shape after Reshape:\", values.shape)\n",
    "print(\"Keys Shape after Reshape:\", keys.shape)\n",
    "print(\"Queries Shape after Reshape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values Shape after Linear Transformation: torch.Size([1, 10, 2, 5])\n",
      "Keys Shape after Linear Transformation: torch.Size([1, 10, 2, 5])\n",
      "Queries Shape after Linear Transformation: torch.Size([1, 10, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Apply linear transformations to the reshaped embeddings\n",
    "values = values_linear(values)\n",
    "keys = keys_linear(keys)\n",
    "queries = queries_linear(queries)\n",
    "\n",
    "# The reshaped embeddings are projected into different vector spaces (query, key, and value)\n",
    "# This helps in calculating attention scores and obtaining the relevant context for each word\n",
    "\n",
    "print(\"Values Shape after Linear Transformation:\", values.shape)\n",
    "print(\"Keys Shape after Linear Transformation:\", keys.shape)\n",
    "print(\"Queries Shape after Linear Transformation:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Shape: torch.Size([1, 10, 2, 2])\n",
      "Scores: tensor([[[[-0.1030, -0.2223],\n",
      "          [-0.3210, -0.6317]],\n",
      "\n",
      "         [[-0.2229, -0.1614],\n",
      "          [ 0.2494,  0.2484]],\n",
      "\n",
      "         [[ 0.1188,  0.0469],\n",
      "          [ 0.5596, -0.2194]],\n",
      "\n",
      "         [[-0.2212,  0.0894],\n",
      "          [ 0.1758, -0.3274]],\n",
      "\n",
      "         [[ 0.1100, -0.2067],\n",
      "          [-0.1585,  0.0536]],\n",
      "\n",
      "         [[-0.1201, -0.1736],\n",
      "          [ 0.4840, -0.1881]],\n",
      "\n",
      "         [[ 0.0022, -0.0563],\n",
      "          [ 0.0427,  0.0560]],\n",
      "\n",
      "         [[-0.0377, -0.1694],\n",
      "          [ 0.3752, -0.2123]],\n",
      "\n",
      "         [[-0.1618, -0.6680],\n",
      "          [-0.3269, -0.1881]],\n",
      "\n",
      "         [[ 0.0133, -0.1859],\n",
      "          [-0.0267,  0.0188]]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Scaled dot-product attention\n",
    "d_k = queries.size(-1)\n",
    "# d_k is the dimension of the keys, used for scaling the dot product to prevent large values\n",
    "\n",
    "scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "# scores are calculated as the dot product of queries and keys, scaled by the square root of the key dimension\n",
    "\n",
    "print(\"Scores Shape:\", scores.shape)\n",
    "print(\"Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Shape: torch.Size([1, 10, 2, 2])\n",
      "Attention Weights: tensor([[[[0.5298, 0.4702],\n",
      "          [0.5771, 0.4229]],\n",
      "\n",
      "         [[0.4846, 0.5154],\n",
      "          [0.5003, 0.4997]],\n",
      "\n",
      "         [[0.5180, 0.4820],\n",
      "          [0.6855, 0.3145]],\n",
      "\n",
      "         [[0.4230, 0.5770],\n",
      "          [0.6232, 0.3768]],\n",
      "\n",
      "         [[0.5785, 0.4215],\n",
      "          [0.4472, 0.5528]],\n",
      "\n",
      "         [[0.5134, 0.4866],\n",
      "          [0.6620, 0.3380]],\n",
      "\n",
      "         [[0.5146, 0.4854],\n",
      "          [0.4967, 0.5033]],\n",
      "\n",
      "         [[0.5329, 0.4671],\n",
      "          [0.6428, 0.3572]],\n",
      "\n",
      "         [[0.6239, 0.3761],\n",
      "          [0.4653, 0.5347]],\n",
      "\n",
      "         [[0.5496, 0.4504],\n",
      "          [0.4886, 0.5114]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to get attention weights\n",
    "attention = torch.nn.functional.softmax(scores, dim=-1)\n",
    "# The attention weights are calculated using the softmax function to normalize the scores\n",
    "\n",
    "print(\"Attention Shape:\", attention.shape)\n",
    "print(\"Attention Weights:\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([1, 10, 2, 5])\n",
      "Output: tensor([[[[ 5.1275e-01, -2.3062e-02,  7.6786e-01,  3.5396e-01, -4.8948e-01],\n",
      "          [ 4.7629e-01,  5.5857e-03,  7.1325e-01,  3.1193e-01, -4.6188e-01]],\n",
      "\n",
      "         [[ 6.8100e-01,  2.7311e-01,  1.4495e-01, -1.7394e-01, -8.7010e-01],\n",
      "          [ 6.8551e-01,  2.7291e-01,  1.5859e-01, -1.7169e-01, -8.5177e-01]],\n",
      "\n",
      "         [[-4.1000e-01,  6.4581e-01, -5.4488e-01, -5.8096e-01,  5.3965e-02],\n",
      "          [-3.2036e-01,  5.2982e-01, -2.9135e-01, -4.3456e-01,  2.2621e-01]],\n",
      "\n",
      "         [[-2.7894e-02,  2.6303e-01, -7.1920e-02, -2.4139e-01,  2.3254e-01],\n",
      "          [ 1.0286e-01, -4.0710e-04,  2.4272e-01,  9.5863e-02,  6.4063e-02]],\n",
      "\n",
      "         [[ 1.7366e-01, -1.0080e-01,  4.3616e-01,  3.5210e-01, -4.7065e-01],\n",
      "          [ 6.3986e-02,  7.8689e-02,  2.7882e-01,  1.4916e-01, -3.1616e-01]],\n",
      "\n",
      "         [[ 9.2607e-02, -1.6513e-01,  3.7339e-01,  1.7588e-01,  4.9838e-01],\n",
      "          [ 9.3334e-02, -1.9488e-01,  3.7261e-01,  1.9396e-01,  4.7630e-01]],\n",
      "\n",
      "         [[ 8.2028e-01, -8.0415e-02,  5.8902e-01,  1.7343e-01, -4.5758e-01],\n",
      "          [ 8.2184e-01, -7.6617e-02,  5.8630e-01,  1.6703e-01, -4.5077e-01]],\n",
      "\n",
      "         [[ 5.8257e-01,  2.1398e-01,  3.2396e-01, -1.9416e-01, -1.7469e-02],\n",
      "          [ 6.0997e-01,  1.6047e-01,  3.1681e-01, -1.5881e-01, -1.0710e-01]],\n",
      "\n",
      "         [[ 5.2244e-01,  6.7247e-01, -3.1140e-02, -6.9942e-01,  1.7911e-01],\n",
      "          [ 4.9562e-01,  6.1047e-01, -1.7423e-02, -5.9421e-01, -1.9935e-02]],\n",
      "\n",
      "         [[ 8.5642e-02, -4.1745e-02,  1.4931e-01,  8.6154e-02,  1.2296e-02],\n",
      "          [ 5.7735e-02, -1.1539e-02,  1.0940e-01,  6.4047e-02, -2.6281e-02]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the output as a weighted sum of values\n",
    "output = torch.matmul(attention, values)\n",
    "# The output is calculated as the weighted sum of the values, where the weights are the attention weights\n",
    "\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output Shape: torch.Size([1, 10, 10])\n",
      "Final Output: tensor([[[ 4.3352e-01,  6.4582e-02, -2.0290e-01, -1.3944e-01, -4.8964e-01,\n",
      "           6.4467e-03, -2.5833e-01, -4.8392e-02,  1.8122e-01, -4.5369e-01],\n",
      "         [ 1.9231e-01, -2.8036e-01,  2.4081e-01, -1.3562e-03,  5.1849e-01,\n",
      "           8.9808e-03,  3.7103e-01, -1.3844e-01, -2.2998e-01, -9.1597e-02],\n",
      "         [ 2.3325e-01,  5.7716e-02,  3.2724e-01,  3.4075e-02, -1.0252e-01,\n",
      "          -6.5169e-02,  1.2867e-01, -1.2793e-01, -2.2600e-01,  4.2445e-02],\n",
      "         [ 5.0482e-01,  1.6524e-02,  5.1617e-02, -7.5409e-02, -3.3103e-01,\n",
      "          -1.4219e-01, -1.0279e-01, -1.8948e-01,  2.8319e-02, -1.3019e-01],\n",
      "         [ 4.6103e-01, -1.0390e-01,  3.3408e-01, -2.1866e-01,  7.6646e-02,\n",
      "          -2.5859e-01,  1.1607e-01, -2.0052e-01, -1.4547e-01,  1.2353e-01],\n",
      "         [ 4.3236e-01,  5.5618e-02, -1.9086e-01, -1.4108e-01, -4.5655e-01,\n",
      "           3.0693e-03, -2.3673e-01, -4.7470e-02,  1.7033e-01, -4.4062e-01],\n",
      "         [ 2.7380e-01, -1.1220e-01,  3.4897e-01, -8.5482e-02,  3.0467e-01,\n",
      "          -7.5912e-02,  3.0771e-01, -6.2583e-02, -2.3051e-01,  2.6022e-02],\n",
      "         [ 2.3316e-01,  4.4131e-02,  3.6673e-01,  1.4711e-02,  1.0517e-04,\n",
      "          -6.6118e-02,  2.0641e-01, -1.1268e-01, -2.4833e-01,  7.9360e-02],\n",
      "         [ 5.0494e-01,  3.0567e-02,  4.7514e-02, -1.0566e-01, -3.6999e-01,\n",
      "          -1.5165e-01, -1.1815e-01, -1.6956e-01,  4.3213e-02, -1.4625e-01],\n",
      "         [ 4.1134e-01, -1.3207e-01,  2.6191e-01, -2.1619e-01,  5.1732e-02,\n",
      "          -2.4118e-01,  5.9468e-02, -2.1053e-01, -1.3901e-01,  5.4877e-02]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Combine heads and apply final linear transformation\n",
    "output = output.transpose(1, 2).contiguous().view(N, -1, heads * head_dim)\n",
    "# The heads are combined by transposing and reshaping the output to merge the heads\n",
    "\n",
    "fc_out = nn.Linear(heads * head_dim, embedding_size)\n",
    "# A final linear layer is applied to project the combined heads back to the original embedding size\n",
    "\n",
    "output = fc_out(output)\n",
    "\n",
    "print(\"Final Output Shape:\", output.shape)\n",
    "print(\"Final Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
