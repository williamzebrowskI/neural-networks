{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanism\n",
    "\n",
    "The self-attention mechanism allows the model to weigh the importance of different words in a sequence. It helps the model focus on relevant words while encoding a particular word. This is a critical part of the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prior module, We left off with the position encoding embedding. To continue this process, we are pushing the positional embedding into the next compontent of the transformer, the attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoded Embeddings Shape: torch.Size([1, 10, 10])\n",
      "tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
      "           0.4198,  2.7427,  0.2844],\n",
      "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
      "           0.4085,  1.3560, -1.3176],\n",
      "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
      "          -0.6907, -0.4066,  1.2899],\n",
      "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
      "           0.9153, -1.1734,  1.1989],\n",
      "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
      "           0.6083,  0.2095,  1.5739],\n",
      "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
      "           0.2480, -0.2364,  0.8417],\n",
      "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
      "           1.8013,  0.7900, -0.9430],\n",
      "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
      "           1.8325,  0.3009,  1.0083],\n",
      "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
      "           0.6484,  0.9932,  0.4030],\n",
      "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
      "          -0.8118,  0.3455,  0.9474]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Positional Encoded Embeddings from the previous notebook\n",
    "pos_encoded_embeddings = torch.tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
    "           0.4198,  2.7427,  0.2844],\n",
    "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
    "           0.4085,  1.3560, -1.3176],\n",
    "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
    "          -0.6907, -0.4066,  1.2899],\n",
    "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
    "           0.9153, -1.1734,  1.1989],\n",
    "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
    "           0.6083,  0.2095,  1.5739],\n",
    "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
    "           0.2480, -0.2364,  0.8417],\n",
    "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
    "           1.8013,  0.7900, -0.9430],\n",
    "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
    "           1.8325,  0.3009,  1.0083],\n",
    "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
    "           0.6484,  0.9932,  0.4030],\n",
    "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
    "          -0.8118,  0.3455,  0.9474]]], dtype=torch.float)\n",
    "\n",
    "print(\"Positional Encoded Embeddings Shape:\", pos_encoded_embeddings.shape) # torch.Size([1, 10, 10]) because we have 1 batch, 10 tokens, and 10 features\n",
    "print(pos_encoded_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformation to Generate Queries, Keys, and Values\n",
    "\n",
    "The positional encoded embeddings are passed through three different linear layers to generate the Query (Q), Key (K), and Value (V) matrices. These matrices are used to compute the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries Shape: torch.Size([1, 10, 10])\n",
      "Queries Matrix:\n",
      " tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
      "           0.4198,  2.7427,  0.2844],\n",
      "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
      "           0.4085,  1.3560, -1.3176],\n",
      "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
      "          -0.6907, -0.4066,  1.2899],\n",
      "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
      "           0.9153, -1.1734,  1.1989],\n",
      "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
      "           0.6083,  0.2095,  1.5739],\n",
      "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
      "           0.2480, -0.2364,  0.8417],\n",
      "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
      "           1.8013,  0.7900, -0.9430],\n",
      "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
      "           1.8325,  0.3009,  1.0083],\n",
      "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
      "           0.6484,  0.9932,  0.4030],\n",
      "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
      "          -0.8118,  0.3455,  0.9474]]])\n",
      "Keys Shape: torch.Size([1, 10, 10])\n",
      "Keys Matrix:\n",
      " tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
      "           0.4198,  2.7427,  0.2844],\n",
      "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
      "           0.4085,  1.3560, -1.3176],\n",
      "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
      "          -0.6907, -0.4066,  1.2899],\n",
      "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
      "           0.9153, -1.1734,  1.1989],\n",
      "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
      "           0.6083,  0.2095,  1.5739],\n",
      "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
      "           0.2480, -0.2364,  0.8417],\n",
      "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
      "           1.8013,  0.7900, -0.9430],\n",
      "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
      "           1.8325,  0.3009,  1.0083],\n",
      "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
      "           0.6484,  0.9932,  0.4030],\n",
      "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
      "          -0.8118,  0.3455,  0.9474]]])\n",
      "Values Shape: torch.Size([1, 10, 10])\n",
      "Values Matrix:\n",
      " tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
      "           0.4198,  2.7427,  0.2844],\n",
      "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
      "           0.4085,  1.3560, -1.3176],\n",
      "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
      "          -0.6907, -0.4066,  1.2899],\n",
      "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
      "           0.9153, -1.1734,  1.1989],\n",
      "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
      "           0.6083,  0.2095,  1.5739],\n",
      "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
      "           0.2480, -0.2364,  0.8417],\n",
      "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
      "           1.8013,  0.7900, -0.9430],\n",
      "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
      "           1.8325,  0.3009,  1.0083],\n",
      "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
      "           0.6484,  0.9932,  0.4030],\n",
      "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
      "          -0.8118,  0.3455,  0.9474]]])\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "d_model = pos_encoded_embeddings.size(1)\n",
    "d_k = d_model  # Assuming d_k = d_model for simplicity\n",
    "\n",
    "# Define weight matrices for linear transformations (for simplicity, we use identity matrices)\n",
    "W_q = torch.eye(d_model)\n",
    "W_k = torch.eye(d_model)\n",
    "W_v = torch.eye(d_model)\n",
    "\n",
    "# Linear transformations to generate Q, K, V\n",
    "queries = torch.matmul(pos_encoded_embeddings, W_q)\n",
    "keys = torch.matmul(pos_encoded_embeddings, W_k)\n",
    "values = torch.matmul(pos_encoded_embeddings, W_v)\n",
    "\n",
    "print(\"Queries Shape:\", queries.shape)\n",
    "print(\"Queries Matrix:\\n\", queries)\n",
    "print(\"Keys Shape:\", keys.shape)\n",
    "print(\"Keys Matrix:\\n\", keys)\n",
    "print(\"Values Shape:\", values.shape)\n",
    "print(\"Values Matrix:\\n\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "To compute the attention scores, we take the dot product of the Query and Key matrices. The result is then scaled by the square root of the dimension of the keys (\\(d_k\\)) to prevent the values from becoming too large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squre root of the dim of keys: 3.1622776601683795\n",
      "Dot Product of Queries and Keys.Transposed (Scaled):\n",
      "tensor([[[ 3.6524,  1.1905, -0.4707, -1.4006,  0.9222, -0.0992,  0.5557,\n",
      "           0.9713,  0.4132, -0.4219],\n",
      "         [ 1.1905,  4.4182,  1.0308,  0.0706, -0.2009, -1.0589,  2.5619,\n",
      "           1.2458,  3.7226,  0.4597],\n",
      "         [-0.4707,  1.0308,  3.8937,  2.1286,  0.1242,  0.4726, -0.6264,\n",
      "           0.2723,  1.7791,  1.4722],\n",
      "         [-1.4006,  0.0706,  2.1286,  3.6419,  1.0043,  1.2146,  0.8493,\n",
      "           1.3136,  0.4297,  1.2584],\n",
      "         [ 0.9222, -0.2009,  0.1242,  1.0043,  4.0949, -0.9581,  0.4650,\n",
      "          -0.3064, -1.8694, -0.3491],\n",
      "         [-0.0992, -1.0589,  0.4726,  1.2146, -0.9581,  2.5267, -0.5795,\n",
      "           2.3920,  0.0560,  0.9574],\n",
      "         [ 0.5557,  2.5619, -0.6264,  0.8493,  0.4650, -0.5795,  3.0812,\n",
      "           1.2700,  2.0481, -0.0433],\n",
      "         [ 0.9713,  1.2458,  0.2723,  1.3136, -0.3064,  2.3920,  1.2700,\n",
      "           5.6132,  2.3743,  0.7503],\n",
      "         [ 0.4132,  3.7226,  1.7791,  0.4297, -1.8694,  0.0559,  2.0481,\n",
      "           2.3743,  6.4663,  1.4786],\n",
      "         [-0.4219,  0.4597,  1.4722,  1.2584, -0.3491,  0.9574, -0.0433,\n",
      "           0.7503,  1.4786,  1.6644]]])\n",
      "Scores Shape: torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Compute the dot product of queries and keys, then scale\n",
    "print(f\"squre root of the dim of keys: {math.sqrt(d_k)}\")\n",
    "\n",
    "scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "print(\"Dot Product of Queries and Keys.Transposed (Scaled):\")\n",
    "print(scores)\n",
    "print(\"Scores Shape:\", scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax to Get Attention Weights\n",
    "\n",
    "The scaled dot-product scores are then passed through a softmax function to convert them into probabilities. These probabilities represent the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights Shape: torch.Size([1, 10, 10])\n",
      "Attention Weights:\n",
      " tensor([[[7.3184e-01, 6.2401e-02, 1.1852e-02, 4.6762e-03, 4.7721e-02,\n",
      "          1.7182e-02, 3.3077e-02, 5.0123e-02, 2.8682e-02, 1.2444e-02],\n",
      "         [2.1827e-02, 5.5053e-01, 1.8606e-02, 7.1228e-03, 5.4294e-03,\n",
      "          2.3020e-03, 8.6020e-02, 2.3070e-02, 2.7458e-01, 1.0511e-02],\n",
      "         [8.2410e-03, 3.6988e-02, 6.4774e-01, 1.1087e-01, 1.4939e-02,\n",
      "          2.1167e-02, 7.0524e-03, 1.7324e-02, 7.8172e-02, 5.7510e-02],\n",
      "         [3.7856e-03, 1.6485e-02, 1.2908e-01, 5.8624e-01, 4.1934e-02,\n",
      "          5.1750e-02, 3.5914e-02, 5.7137e-02, 2.3607e-02, 5.4067e-02],\n",
      "         [3.5522e-02, 1.1554e-02, 1.5992e-02, 3.8558e-02, 8.4793e-01,\n",
      "          5.4186e-03, 2.2487e-02, 1.0397e-02, 2.1783e-03, 9.9621e-03],\n",
      "         [2.6416e-02, 1.0118e-02, 4.6799e-02, 9.8277e-02, 1.1191e-02,\n",
      "          3.6500e-01, 1.6342e-02, 3.1902e-01, 3.0851e-02, 7.5989e-02],\n",
      "         [3.2410e-02, 2.4096e-01, 9.9376e-03, 4.3468e-02, 2.9600e-02,\n",
      "          1.0415e-02, 4.0505e-01, 6.6205e-02, 1.4415e-01, 1.7805e-02],\n",
      "         [8.4322e-03, 1.1096e-02, 4.1915e-03, 1.1874e-02, 2.3497e-03,\n",
      "          3.4909e-02, 1.1367e-02, 8.7473e-01, 3.4296e-02, 6.7602e-03],\n",
      "         [2.1066e-03, 5.7655e-02, 8.2567e-03, 2.1417e-03, 2.1492e-04,\n",
      "          1.4738e-03, 1.0805e-02, 1.4972e-02, 8.9626e-01, 6.1138e-03],\n",
      "         [2.5056e-02, 6.0506e-02, 1.6653e-01, 1.3448e-01, 2.6948e-02,\n",
      "          9.9525e-02, 3.6589e-02, 8.0913e-02, 1.6762e-01, 2.0183e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to the scores to get the attention weights\n",
    "attention = torch.nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "print(\"Attention Weights Shape:\", attention.shape)\n",
    "print(\"Attention Weights:\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Sum of Values to Get the Output\n",
    "\n",
    "The final step in the attention mechanism is to compute a weighted sum of the Value matrix using the attention weights. This gives us the output of the self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights:\n",
      "tensor([[[7.3184e-01, 6.2401e-02, 1.1852e-02, 4.6762e-03, 4.7721e-02,\n",
      "          1.7182e-02, 3.3077e-02, 5.0123e-02, 2.8682e-02, 1.2444e-02],\n",
      "         [2.1827e-02, 5.5053e-01, 1.8606e-02, 7.1228e-03, 5.4294e-03,\n",
      "          2.3020e-03, 8.6020e-02, 2.3070e-02, 2.7458e-01, 1.0511e-02],\n",
      "         [8.2410e-03, 3.6988e-02, 6.4774e-01, 1.1087e-01, 1.4939e-02,\n",
      "          2.1167e-02, 7.0524e-03, 1.7324e-02, 7.8172e-02, 5.7510e-02],\n",
      "         [3.7856e-03, 1.6485e-02, 1.2908e-01, 5.8624e-01, 4.1934e-02,\n",
      "          5.1750e-02, 3.5914e-02, 5.7137e-02, 2.3607e-02, 5.4067e-02],\n",
      "         [3.5522e-02, 1.1554e-02, 1.5992e-02, 3.8558e-02, 8.4793e-01,\n",
      "          5.4186e-03, 2.2487e-02, 1.0397e-02, 2.1783e-03, 9.9621e-03],\n",
      "         [2.6416e-02, 1.0118e-02, 4.6799e-02, 9.8277e-02, 1.1191e-02,\n",
      "          3.6500e-01, 1.6342e-02, 3.1902e-01, 3.0851e-02, 7.5989e-02],\n",
      "         [3.2410e-02, 2.4096e-01, 9.9376e-03, 4.3468e-02, 2.9600e-02,\n",
      "          1.0415e-02, 4.0505e-01, 6.6205e-02, 1.4415e-01, 1.7805e-02],\n",
      "         [8.4322e-03, 1.1096e-02, 4.1915e-03, 1.1874e-02, 2.3497e-03,\n",
      "          3.4909e-02, 1.1367e-02, 8.7473e-01, 3.4296e-02, 6.7602e-03],\n",
      "         [2.1066e-03, 5.7655e-02, 8.2567e-03, 2.1417e-03, 2.1492e-04,\n",
      "          1.4738e-03, 1.0805e-02, 1.4972e-02, 8.9626e-01, 6.1138e-03],\n",
      "         [2.5056e-02, 6.0506e-02, 1.6653e-01, 1.3448e-01, 2.6948e-02,\n",
      "          9.9525e-02, 3.6589e-02, 8.0913e-02, 1.6762e-01, 2.0183e-01]]])\n",
      "\n",
      "values matrix: \n",
      "tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
      "           0.4198,  2.7427,  0.2844],\n",
      "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
      "           0.4085,  1.3560, -1.3176],\n",
      "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
      "          -0.6907, -0.4066,  1.2899],\n",
      "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
      "           0.9153, -1.1734,  1.1989],\n",
      "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
      "           0.6083,  0.2095,  1.5739],\n",
      "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
      "           0.2480, -0.2364,  0.8417],\n",
      "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
      "           1.8013,  0.7900, -0.9430],\n",
      "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
      "           1.8325,  0.3009,  1.0083],\n",
      "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
      "           0.6484,  0.9932,  0.4030],\n",
      "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
      "          -0.8118,  0.3455,  0.9474]]])\n",
      "\n",
      "Output Shape: torch.Size([1, 10, 10])\n",
      "Output:\n",
      " tensor([[[ 0.0600,  0.7737,  0.6171,  0.9138,  0.4270, -0.2455,  0.4905,\n",
      "           0.5220,  2.1615,  0.2791],\n",
      "         [ 0.9690, -0.5384,  2.0179,  0.7043,  0.0409,  1.7661, -0.1429,\n",
      "           0.5983,  1.1423, -0.6134],\n",
      "         [-0.2322,  0.0513,  0.5940,  0.2128,  1.0213,  2.2142, -0.4291,\n",
      "          -0.2646, -0.2143,  1.0602],\n",
      "         [-0.6043, -0.1779, -0.0352,  0.8562,  0.1502,  1.6106, -1.0049,\n",
      "           0.6349, -0.6234,  1.0427],\n",
      "         [-0.3628, -0.6022, -1.0031,  1.8144, -0.1798,  0.1831,  1.5708,\n",
      "           0.6139,  0.2642,  1.4004],\n",
      "         [-0.7475,  1.0211,  0.9330,  0.3515, -0.6321,  0.6881, -1.1458,\n",
      "           0.7425,  0.0337,  0.8879],\n",
      "         [ 0.6322, -0.5580,  1.2409,  0.9653, -0.2560,  1.1861, -0.3562,\n",
      "           1.0955,  0.8536, -0.4282],\n",
      "         [-0.3073,  1.8349,  1.7938,  0.6599, -1.5487,  0.8422, -0.9513,\n",
      "           1.6663,  0.3234,  0.9320],\n",
      "         [ 1.4697, -0.7707,  3.1002, -0.3606,  0.1453,  1.8283, -0.9233,\n",
      "           0.6443,  0.9831,  0.3113],\n",
      "         [-0.1339, -0.0335,  0.9703,  0.3132,  0.0657,  1.4162, -0.7874,\n",
      "           0.2434,  0.1968,  0.7355]]])\n"
     ]
    }
   ],
   "source": [
    "# Compute the output as a weighted sum of the values\n",
    "print(\"attention weights:\")\n",
    "print(f\"{attention}\\n\")\n",
    "print(\"values matrix: \")\n",
    "print(f\"{values}\\n\")\n",
    "\n",
    "output = torch.matmul(attention, values)\n",
    "\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. **Weighted Sum of Values to Get the Output**\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "After computing the attention weights, the next step is to calculate the final output by taking a weighted sum of the value vectors. This is done through matrix multiplication of the attention weights matrix with the value matrix.\n",
    "\n",
    "Given:\n",
    "- **Attention Weights** matrix $\\mathbf{A}$ of shape $(\\text{number of tokens}, \\text{number of tokens})$.\n",
    "- **Values** matrix $\\mathbf{V}$ of shape $(\\text{number of tokens}, d_{\\text{model}})$.\n",
    "\n",
    "The output is computed as:\n",
    "$$\n",
    "\\mathbf{O} = \\mathbf{A} \\times \\mathbf{V}\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbf{O}$ is the output matrix of shape $(\\text{number of tokens}, d_{\\text{model}})$.\n",
    "\n",
    "#### Example Code with Matrix Multiplication\n",
    "\n",
    "```python\n",
    "# Let's assume 3 tokens and a model dimension of 4 for simplicity\n",
    "values = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "], dtype=torch.float)\n",
    "\n",
    "attention_weights = torch.tensor([\n",
    "    [0.2, 0.3, 0.5],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.4, 0.1, 0.5]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Perform the matrix multiplication\n",
    "output = torch.matmul(attention_weights, values)\n",
    "\n",
    "print(\"Attention Weights Matrix (A):\\n\", attention_weights)\n",
    "print(\"Values Matrix (V):\\n\", values)\n",
    "print(\"Output Matrix (O = A * V):\\n\", output)\n",
    "```\n",
    "\n",
    "#### Expected Output\n",
    "\n",
    "Given the example matrices:\n",
    "\n",
    "- **Attention Weights (A):**\n",
    "  $$\n",
    "  \\mathbf{A} =\n",
    "  \\begin{bmatrix}\n",
    "  0.2 & 0.3 & 0.5 \\\\\n",
    "  0.1 & 0.8 & 0.1 \\\\\n",
    "  0.4 & 0.1 & 0.5\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Values (V):**\n",
    "  $$\n",
    "  \\mathbf{V} =\n",
    "  \\begin{bmatrix}\n",
    "  0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
    "  0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
    "  0.9 & 1.0 & 1.1 & 1.2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "The output matrix $ \\mathbf{O} $ will be:\n",
    "\n",
    "- **Output (O):**\n",
    "  $$\n",
    "  \\mathbf{O} =\n",
    "  \\begin{bmatrix}\n",
    "  (0.2 \\times 0.1 + 0.3 \\times 0.5 + 0.5 \\times 0.9) & (0.2 \\times 0.2 + 0.3 \\times 0.6 + 0.5 \\times 1.0) & (0.2 \\times 0.3 + 0.3 \\times 0.7 + 0.5 \\times 1.1) & (0.2 \\times 0.4 + 0.3 \\times 0.8 + 0.5 \\times 1.2) \\\\\n",
    "  (0.1 \\times 0.1 + 0.8 \\times 0.5 + 0.1 \\times 0.9) & (0.1 \\times 0.2 + 0.8 \\times 0.6 + 0.1 \\times 1.0) & (0.1 \\times 0.3 + 0.8 \\times 0.7 + 0.1 \\times 1.1) & (0.1 \\times 0.4 + 0.8 \\times 0.8 + 0.1 \\times 1.2) \\\\\n",
    "  (0.4 \\times 0.1 + 0.1 \\times 0.5 + 0.5 \\times 0.9) & (0.4 \\times 0.2 + 0.1 \\times 0.6 + 0.5 \\times 1.0) & (0.4 \\times 0.3 + 0.1 \\times 0.7 + 0.5 \\times 1.1) & (0.4 \\times 0.4 + 0.1 \\times 0.8 + 0.5 \\times 1.2)\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "Evaluating the above gives:\n",
    "\n",
    "$$\n",
    "\\mathbf{O} =\n",
    "\\begin{bmatrix}\n",
    "0.62 & 0.74 & 0.86 & 0.98 \\\\\n",
    "0.42 & 0.54 & 0.66 & 0.78 \\\\\n",
    "0.64 & 0.76 & 0.88 & 1.00\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, the output matrix $ \\mathbf{O} $ will be:\n",
    "\n",
    "```\n",
    "tensor([[0.62, 0.74, 0.86, 0.98],\n",
    "        [0.42, 0.54, 0.66, 0.78],\n",
    "        [0.64, 0.76, 0.88, 1.00]])\n",
    "```\n",
    "\n",
    "This demonstrates how each element in the output matrix is a weighted sum of the corresponding value matrix, with weights given by the attention scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Heads and Final Linear Transformation\n",
    "\n",
    "In a multi-head attention mechanism, multiple sets of Q, K, V matrices are computed and processed in parallel. These are then concatenated and passed through a final linear transformation to project them back to the original embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output Shape: torch.Size([1, 10, 10])\n",
      "Final Output:\n",
      " tensor([[[-8.4285e-01, -1.7612e-01, -2.7030e-01,  4.4906e-01,  7.3453e-01,\n",
      "           6.4252e-01, -7.2729e-02, -6.9949e-01,  1.0482e+00,  9.9681e-01],\n",
      "         [ 8.2112e-02, -4.4043e-01, -3.8180e-01, -8.5722e-01,  7.8969e-02,\n",
      "           1.3080e+00,  1.9322e-01,  4.9748e-02,  8.4398e-01,  1.1881e+00],\n",
      "         [-1.6093e-01,  2.5082e-01,  7.8345e-01,  4.3364e-02,  2.3881e-01,\n",
      "           2.3484e-01, -3.1012e-01,  1.4730e-01,  6.2287e-01,  1.5049e-01],\n",
      "         [-6.8474e-04,  4.7168e-02,  3.6157e-01, -1.2725e-01,  2.2440e-01,\n",
      "           2.7673e-01, -1.4018e-01,  3.3133e-01, -7.4699e-02, -9.5472e-02],\n",
      "         [-4.7937e-01,  1.9969e-01,  3.9888e-01,  8.8724e-01,  9.8833e-01,\n",
      "           2.3337e-01,  4.5606e-01, -5.3211e-01, -2.1268e-01,  9.2721e-01],\n",
      "         [ 6.8733e-02,  1.3531e-01, -1.7628e-01, -1.8989e-01, -9.0967e-03,\n",
      "           3.9240e-01, -1.1892e-01, -2.8764e-01,  5.2636e-01, -3.7702e-01],\n",
      "         [ 1.6201e-02, -4.6819e-01, -4.4914e-01, -7.6081e-01,  1.1173e-01,\n",
      "           1.1119e+00,  1.9018e-01,  9.1876e-02,  4.0922e-01,  9.6834e-01],\n",
      "         [-1.0435e-01,  1.7367e-01, -4.9674e-01, -6.6554e-01, -4.5320e-01,\n",
      "           8.7088e-01, -8.9589e-02, -4.7829e-01,  9.1668e-01,  4.2451e-02],\n",
      "         [ 6.3274e-01, -6.7152e-01, -1.8436e-01, -1.5552e+00, -2.8444e-01,\n",
      "           1.0424e+00, -4.3299e-01, -3.6836e-01,  1.1609e+00,  9.9004e-01],\n",
      "         [ 1.5846e-01, -2.7371e-02,  1.3795e-01, -3.2816e-01,  2.0249e-01,\n",
      "           4.4698e-01, -1.3673e-01, -1.1383e-01,  5.0143e-01,  1.1449e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example with one head (for simplicity)\n",
    "N, heads, head_dim, embedding_size = 1, 1, d_model, d_model\n",
    "\n",
    "# Combine heads (for demonstration, we consider one head only)\n",
    "output = output.view(N, -1, heads * head_dim)\n",
    "\n",
    "# Final linear transformation to project back to the original embedding size\n",
    "fc_out = nn.Linear(heads * head_dim, embedding_size)\n",
    "output = fc_out(output)\n",
    "\n",
    "print(\"Final Output Shape:\", output.shape)\n",
    "print(\"Final Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
