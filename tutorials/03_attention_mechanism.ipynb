{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanism\n",
    "\n",
    "The self-attention mechanism allows the model to weigh the importance of different words in a sequence. It helps the model focus on relevant words while encoding a particular word. This is a critical part of the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prior module, We left off with the position encoding embedding. To continue this process, we are pushing the positional embedding into the next compontent of the transformer, the attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoded Embeddings Shape: torch.Size([1, 10, 10])\n",
      "tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
      "           0.4198,  2.7427,  0.2844],\n",
      "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
      "           0.4085,  1.3560, -1.3176],\n",
      "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
      "          -0.6907, -0.4066,  1.2899],\n",
      "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
      "           0.9153, -1.1734,  1.1989],\n",
      "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
      "           0.6083,  0.2095,  1.5739],\n",
      "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
      "           0.2480, -0.2364,  0.8417],\n",
      "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
      "           1.8013,  0.7900, -0.9430],\n",
      "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
      "           1.8325,  0.3009,  1.0083],\n",
      "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
      "           0.6484,  0.9932,  0.4030],\n",
      "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
      "          -0.8118,  0.3455,  0.9474]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Positional Encoded Embeddings from the previous notebook\n",
    "pos_encoded_embeddings = torch.tensor([[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193, -0.7430,  0.7120,\n",
    "           0.4198,  2.7427,  0.2844],\n",
    "         [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651,  2.0396,  0.3780,\n",
    "           0.4085,  1.3560, -1.3176],\n",
    "         [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146,  2.6475, -0.1887,\n",
    "          -0.6907, -0.4066,  1.2899],\n",
    "         [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762,  1.8148, -1.4084,\n",
    "           0.9153, -1.1734,  1.1989],\n",
    "         [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399,  0.0412,  1.9375,\n",
    "           0.6083,  0.2095,  1.5739],\n",
    "         [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064, -0.0906, -1.6746,\n",
    "           0.2480, -0.2364,  0.8417],\n",
    "         [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185,  0.6701, -0.5353,\n",
    "           1.8013,  0.7900, -0.9430],\n",
    "         [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692,  0.8200, -0.9620,\n",
    "           1.8325,  0.3009,  1.0083],\n",
    "         [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798,  1.8522, -1.0186,\n",
    "           0.6484,  0.9932,  0.4030],\n",
    "         [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465,  1.1548, -1.1130,\n",
    "          -0.8118,  0.3455,  0.9474]]], dtype=torch.float)\n",
    "\n",
    "print(\"Positional Encoded Embeddings Shape:\", pos_encoded_embeddings.shape) # torch.Size([1, 10, 10]) because we have 1 batch, 10 tokens, and 10 features\n",
    "print(pos_encoded_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Size: 10\n",
      "Number of Heads: 2\n",
      "Dimension per Head: 5\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions for the model\n",
    "embedding_size = pos_encoded_embeddings.size(2)\n",
    "# embedding_size is the size of the embeddings vector for each word in the sentence\n",
    "heads = 2\n",
    "# heads is the number of attention heads, allowing the model to focus on different parts of the sentence simultaneously\n",
    "head_dim = embedding_size // heads\n",
    "# head_dim is the size of each attention head, meaning each head will output a vector of size head_dim ensuring that the embedding size is evenly divisible by the number of heads\n",
    "\n",
    "print(\"Embedding Size:\", embedding_size)\n",
    "print(\"Number of Heads:\", heads)\n",
    "print(\"Dimension per Head:\", head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=5, bias=False)\n",
      "Linear(in_features=5, out_features=5, bias=False)\n",
      "Linear(in_features=5, out_features=5, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Define linear transformations for query, key, and value\n",
    "values_linear = nn.Linear(head_dim, head_dim, bias=False) # Linear layer for values\n",
    "keys_linear = nn.Linear(head_dim, head_dim, bias=False) # Linear layer for keys\n",
    "queries_linear = nn.Linear(head_dim, head_dim, bias=False) # Linear layer for queries\n",
    "\n",
    "# These linear layers project the input embeddings into different vector spaces (query, key, and value)\n",
    "# This helps in calculating attention scores and obtaining the relevant context for each word\n",
    "\n",
    "print(values_linear)\n",
    "print(keys_linear)\n",
    "print(queries_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "value len: 10\n",
      "key len: 10\n",
      "query len: 10\n",
      "Values Shape after Reshape: torch.Size([1, 10, 2, 5])\n",
      "Keys Shape after Reshape: torch.Size([1, 10, 2, 5])\n",
      "Queries Shape after Reshape: torch.Size([1, 10, 2, 5])\n",
      "tensor([[[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193],\n",
      "          [-0.7430,  0.7120,  0.4198,  2.7427,  0.2844]],\n",
      "\n",
      "         [[ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651],\n",
      "          [ 2.0396,  0.3780,  0.4085,  1.3560, -1.3176]],\n",
      "\n",
      "         [[-0.3461,  0.2526,  0.4120,  0.0398,  1.6146],\n",
      "          [ 2.6475, -0.1887, -0.6907, -0.4066,  1.2899]],\n",
      "\n",
      "         [[-0.8345, -0.4657, -0.5635,  1.1514,  0.1762],\n",
      "          [ 1.8148, -1.4084,  0.9153, -1.1734,  1.1989]],\n",
      "\n",
      "         [[-0.3981, -0.7277, -1.2657,  1.9887, -0.2399],\n",
      "          [ 0.0412,  1.9375,  0.6083,  0.2095,  1.5739]],\n",
      "\n",
      "         [[-1.5240,  1.2359,  0.5596, -0.1529, -0.4064],\n",
      "          [-0.0906, -1.6746,  0.2480, -0.2364,  0.8417]],\n",
      "\n",
      "         [[ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185],\n",
      "          [ 0.6701, -0.5353,  1.8013,  0.7900, -0.9430]],\n",
      "\n",
      "         [[-0.3550,  2.1032,  1.8690,  0.7174, -1.7692],\n",
      "          [ 0.8200, -0.9620,  1.8325,  0.3009,  1.0083]],\n",
      "\n",
      "         [[ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798],\n",
      "          [ 1.8522, -1.0186,  0.6484,  0.9932,  0.4030]],\n",
      "\n",
      "         [[-0.5990, -0.4916,  0.5419, -0.0293, -0.3465],\n",
      "          [ 1.1548, -1.1130, -0.8118,  0.3455,  0.9474]]]])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the input embeddings to split into heads for multi-head attention\n",
    "N = pos_encoded_embeddings.shape[0] \n",
    "print(f\"batch size: {N}\")\n",
    "# N is the batch size, which is the number of sentences being processed at once (1 in this case)\n",
    "\n",
    "value_len, key_len, query_len = pos_encoded_embeddings.shape[1], pos_encoded_embeddings.shape[1], pos_encoded_embeddings.shape[1]\n",
    "print(f\"value len: {value_len}\")\n",
    "print(f\"key len: {key_len}\")\n",
    "print(f\"query len: {query_len}\")\n",
    "# value_len, key_len, and query_len are the lengths of the input sequences in this case (10 in this case)\n",
    "\n",
    "values = pos_encoded_embeddings.reshape(N, value_len, heads, head_dim) # Reshape the input embeddings to split into heads for multi-head attention in this case (1, 10, 2, 5).  1 being the batch size, 10 being the number of tokens, 2 being the number of heads, and 5 being the dimension per head\n",
    "keys = pos_encoded_embeddings.reshape(N, key_len, heads, head_dim) # Reshape the input embeddings to split into heads for multi-head attention in this case (1, 10, 2, 5). 1 being the batch size, 10 being the number of tokens, 2 being the number of heads, and 5 being the dimension per head\n",
    "queries = pos_encoded_embeddings.reshape(N, query_len, heads, head_dim) # Reshape the input embeddings to split into heads for multi-head attention in this case (1, 10, 2, 5). 1 being the batch size, 10 being the number of tokens, 2 being the number of heads, and 5 being the dimension per head\n",
    "# The input embeddings are reshaped to separate the heads for multi-head attention\n",
    "\n",
    "print(\"Values Shape after Reshape:\", values.shape)\n",
    "print(\"Keys Shape after Reshape:\", keys.shape)\n",
    "print(\"Queries Shape after Reshape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values Shape after Linear Transformation: torch.Size([1, 10, 2, 5])\n",
      "Keys Shape after Linear Transformation: torch.Size([1, 10, 2, 5])\n",
      "Queries Shape after Linear Transformation: torch.Size([1, 10, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Apply linear transformations to the reshaped embeddings to project them into different vector spaces\n",
    "values = values_linear(values) # Linear transformation for values\n",
    "keys = keys_linear(keys) # Linear transformation for keys\n",
    "queries = queries_linear(queries) # Linear transformation for queries\n",
    "\n",
    "# The reshaped embeddings are projected into different vector spaces (query, key, and value)\n",
    "# This helps in calculating attention scores and obtaining the relevant context for each word\n",
    "\n",
    "print(\"Values Shape after Linear Transformation:\", values.shape)\n",
    "print(\"Keys Shape after Linear Transformation:\", keys.shape)\n",
    "print(\"Queries Shape after Linear Transformation:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query matrix: \n",
      " tensor([[[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193],\n",
      "          [-0.7430,  0.7120,  0.4198,  2.7427,  0.2844]],\n",
      "\n",
      "         [[ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651],\n",
      "          [ 2.0396,  0.3780,  0.4085,  1.3560, -1.3176]],\n",
      "\n",
      "         [[-0.3461,  0.2526,  0.4120,  0.0398,  1.6146],\n",
      "          [ 2.6475, -0.1887, -0.6907, -0.4066,  1.2899]],\n",
      "\n",
      "         [[-0.8345, -0.4657, -0.5635,  1.1514,  0.1762],\n",
      "          [ 1.8148, -1.4084,  0.9153, -1.1734,  1.1989]],\n",
      "\n",
      "         [[-0.3981, -0.7277, -1.2657,  1.9887, -0.2399],\n",
      "          [ 0.0412,  1.9375,  0.6083,  0.2095,  1.5739]],\n",
      "\n",
      "         [[-1.5240,  1.2359,  0.5596, -0.1529, -0.4064],\n",
      "          [-0.0906, -1.6746,  0.2480, -0.2364,  0.8417]],\n",
      "\n",
      "         [[ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185],\n",
      "          [ 0.6701, -0.5353,  1.8013,  0.7900, -0.9430]],\n",
      "\n",
      "         [[-0.3550,  2.1032,  1.8690,  0.7174, -1.7692],\n",
      "          [ 0.8200, -0.9620,  1.8325,  0.3009,  1.0083]],\n",
      "\n",
      "         [[ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798],\n",
      "          [ 1.8522, -1.0186,  0.6484,  0.9932,  0.4030]],\n",
      "\n",
      "         [[-0.5990, -0.4916,  0.5419, -0.0293, -0.3465],\n",
      "          [ 1.1548, -1.1130, -0.8118,  0.3455,  0.9474]]]]) \n",
      "\n",
      "key transposed matrix: \n",
      " tensor([[[[ 0.0171,  1.0654,  0.4616,  0.9196,  0.7193],\n",
      "          [ 0.9161, -0.4999,  1.8302,  1.2182,  0.0651],\n",
      "          [-0.3461,  0.2526,  0.4120,  0.0398,  1.6146],\n",
      "          [-0.8345, -0.4657, -0.5635,  1.1514,  0.1762],\n",
      "          [-0.3981, -0.7277, -1.2657,  1.9887, -0.2399],\n",
      "          [-1.5240,  1.2359,  0.5596, -0.1529, -0.4064],\n",
      "          [ 0.6992, -1.1193,  0.5642,  1.3861, -0.5185],\n",
      "          [-0.3550,  2.1032,  1.8690,  0.7174, -1.7692],\n",
      "          [ 1.5902, -0.8516,  3.2954, -0.5147,  0.1798],\n",
      "          [-0.5990, -0.4916,  0.5419, -0.0293, -0.3465]],\n",
      "\n",
      "         [[-0.7430,  0.7120,  0.4198,  2.7427,  0.2844],\n",
      "          [ 2.0396,  0.3780,  0.4085,  1.3560, -1.3176],\n",
      "          [ 2.6475, -0.1887, -0.6907, -0.4066,  1.2899],\n",
      "          [ 1.8148, -1.4084,  0.9153, -1.1734,  1.1989],\n",
      "          [ 0.0412,  1.9375,  0.6083,  0.2095,  1.5739],\n",
      "          [-0.0906, -1.6746,  0.2480, -0.2364,  0.8417],\n",
      "          [ 0.6701, -0.5353,  1.8013,  0.7900, -0.9430],\n",
      "          [ 0.8200, -0.9620,  1.8325,  0.3009,  1.0083],\n",
      "          [ 1.8522, -1.0186,  0.6484,  0.9932,  0.4030],\n",
      "          [ 1.1548, -1.1130, -0.8118,  0.3455,  0.9474]]]]) \n",
      "\n",
      "square root of d_k: 2.23606797749979\n",
      "Scores Shape: torch.Size([1, 10, 2, 2])\n",
      "Scores: tensor([[[[ 1.2126,  1.6397],\n",
      "          [ 1.6397,  3.9527]],\n",
      "\n",
      "         [[ 2.6506,  1.7858],\n",
      "          [ 1.7858,  3.5976]],\n",
      "\n",
      "         [[ 1.3246,  0.3658],\n",
      "          [ 0.3658,  4.1819]],\n",
      "\n",
      "         [[ 1.1572, -1.1244],\n",
      "          [-1.1244,  3.9932]],\n",
      "\n",
      "         [[ 2.8186, -0.9647],\n",
      "          [-0.9647,  2.9725]],\n",
      "\n",
      "         [[ 1.9461, -0.9386],\n",
      "          [-0.9386,  1.6271]],\n",
      "\n",
      "         [[ 1.9007,  1.6404],\n",
      "          [ 1.6404,  2.4568]],\n",
      "\n",
      "         [[ 5.2267, -0.2046],\n",
      "          [-0.2046,  2.7115]],\n",
      "\n",
      "         [[ 6.4447,  2.4645],\n",
      "          [ 2.4645,  2.7000]],\n",
      "\n",
      "         [[ 0.4539, -0.4127],\n",
      "          [-0.4127,  1.8999]]]])\n"
     ]
    }
   ],
   "source": [
    "# Scaled dot-product attention\n",
    "d_k = keys.size(-1)\n",
    "# d_k is the dimension of the keys, used for scaling the dot product to prevent large values\n",
    "\n",
    "print(f\"query matrix: \\n {queries} \\n\")\n",
    "print(f\"key transposed matrix: \\n {keys.transpose(-2, 1)} \\n\") # Transpose the keys matrix to perform matrix multiplication\n",
    "print(f\"square root of d_k: {math.sqrt(d_k)}\")\n",
    "\n",
    "\n",
    "scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "# scores are calculated by taking the dot product of the queries and keys, and dividing by the square root of teh dimension of the keys\n",
    "\n",
    "print(\"Scores Shape:\", scores.shape) # In this case, the shape of the scores tensor is (1, 10, 2, 2) \n",
    "print(\"Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Shape: torch.Size([1, 10, 2, 2])\n",
      "Attention Weights: tensor([[[[0.5016, 0.4984],\n",
      "          [0.4048, 0.5952]],\n",
      "\n",
      "         [[0.6133, 0.3867],\n",
      "          [0.3837, 0.6163]],\n",
      "\n",
      "         [[0.4321, 0.5679],\n",
      "          [0.2212, 0.7788]],\n",
      "\n",
      "         [[0.6546, 0.3454],\n",
      "          [0.1494, 0.8506]],\n",
      "\n",
      "         [[0.5803, 0.4197],\n",
      "          [0.4025, 0.5975]],\n",
      "\n",
      "         [[0.5431, 0.4569],\n",
      "          [0.3175, 0.6825]],\n",
      "\n",
      "         [[0.5607, 0.4393],\n",
      "          [0.5563, 0.4437]],\n",
      "\n",
      "         [[0.5641, 0.4359],\n",
      "          [0.1944, 0.8056]],\n",
      "\n",
      "         [[0.6179, 0.3821],\n",
      "          [0.4803, 0.5197]],\n",
      "\n",
      "         [[0.5039, 0.4961],\n",
      "          [0.4792, 0.5208]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to get attention weights\n",
    "attention = torch.nn.functional.softmax(scores, dim=-1)\n",
    "# The attention weights are calculated using the softmax function to normalize the scores\n",
    "\n",
    "print(\"Attention Shape:\", attention.shape)\n",
    "print(\"Attention Weights:\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([1, 10, 2, 5])\n",
      "Output: tensor([[[[ 5.1275e-01, -2.3062e-02,  7.6786e-01,  3.5396e-01, -4.8948e-01],\n",
      "          [ 4.7629e-01,  5.5857e-03,  7.1325e-01,  3.1193e-01, -4.6188e-01]],\n",
      "\n",
      "         [[ 6.8100e-01,  2.7311e-01,  1.4495e-01, -1.7394e-01, -8.7010e-01],\n",
      "          [ 6.8551e-01,  2.7291e-01,  1.5859e-01, -1.7169e-01, -8.5177e-01]],\n",
      "\n",
      "         [[-4.1000e-01,  6.4581e-01, -5.4488e-01, -5.8096e-01,  5.3965e-02],\n",
      "          [-3.2036e-01,  5.2982e-01, -2.9135e-01, -4.3456e-01,  2.2621e-01]],\n",
      "\n",
      "         [[-2.7894e-02,  2.6303e-01, -7.1920e-02, -2.4139e-01,  2.3254e-01],\n",
      "          [ 1.0286e-01, -4.0710e-04,  2.4272e-01,  9.5863e-02,  6.4063e-02]],\n",
      "\n",
      "         [[ 1.7366e-01, -1.0080e-01,  4.3616e-01,  3.5210e-01, -4.7065e-01],\n",
      "          [ 6.3986e-02,  7.8689e-02,  2.7882e-01,  1.4916e-01, -3.1616e-01]],\n",
      "\n",
      "         [[ 9.2607e-02, -1.6513e-01,  3.7339e-01,  1.7588e-01,  4.9838e-01],\n",
      "          [ 9.3334e-02, -1.9488e-01,  3.7261e-01,  1.9396e-01,  4.7630e-01]],\n",
      "\n",
      "         [[ 8.2028e-01, -8.0415e-02,  5.8902e-01,  1.7343e-01, -4.5758e-01],\n",
      "          [ 8.2184e-01, -7.6617e-02,  5.8630e-01,  1.6703e-01, -4.5077e-01]],\n",
      "\n",
      "         [[ 5.8257e-01,  2.1398e-01,  3.2396e-01, -1.9416e-01, -1.7469e-02],\n",
      "          [ 6.0997e-01,  1.6047e-01,  3.1681e-01, -1.5881e-01, -1.0710e-01]],\n",
      "\n",
      "         [[ 5.2244e-01,  6.7247e-01, -3.1140e-02, -6.9942e-01,  1.7911e-01],\n",
      "          [ 4.9562e-01,  6.1047e-01, -1.7423e-02, -5.9421e-01, -1.9935e-02]],\n",
      "\n",
      "         [[ 8.5642e-02, -4.1745e-02,  1.4931e-01,  8.6154e-02,  1.2296e-02],\n",
      "          [ 5.7735e-02, -1.1539e-02,  1.0940e-01,  6.4047e-02, -2.6281e-02]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the output as a weighted sum of values\n",
    "output = torch.matmul(attention, values)\n",
    "# The output is calculated as the weighted sum of the values, where the weights are the attention weights\n",
    "\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output Shape: torch.Size([1, 10, 10])\n",
      "Final Output: tensor([[[ 4.3352e-01,  6.4582e-02, -2.0290e-01, -1.3944e-01, -4.8964e-01,\n",
      "           6.4467e-03, -2.5833e-01, -4.8392e-02,  1.8122e-01, -4.5369e-01],\n",
      "         [ 1.9231e-01, -2.8036e-01,  2.4081e-01, -1.3562e-03,  5.1849e-01,\n",
      "           8.9808e-03,  3.7103e-01, -1.3844e-01, -2.2998e-01, -9.1597e-02],\n",
      "         [ 2.3325e-01,  5.7716e-02,  3.2724e-01,  3.4075e-02, -1.0252e-01,\n",
      "          -6.5169e-02,  1.2867e-01, -1.2793e-01, -2.2600e-01,  4.2445e-02],\n",
      "         [ 5.0482e-01,  1.6524e-02,  5.1617e-02, -7.5409e-02, -3.3103e-01,\n",
      "          -1.4219e-01, -1.0279e-01, -1.8948e-01,  2.8319e-02, -1.3019e-01],\n",
      "         [ 4.6103e-01, -1.0390e-01,  3.3408e-01, -2.1866e-01,  7.6646e-02,\n",
      "          -2.5859e-01,  1.1607e-01, -2.0052e-01, -1.4547e-01,  1.2353e-01],\n",
      "         [ 4.3236e-01,  5.5618e-02, -1.9086e-01, -1.4108e-01, -4.5655e-01,\n",
      "           3.0693e-03, -2.3673e-01, -4.7470e-02,  1.7033e-01, -4.4062e-01],\n",
      "         [ 2.7380e-01, -1.1220e-01,  3.4897e-01, -8.5482e-02,  3.0467e-01,\n",
      "          -7.5912e-02,  3.0771e-01, -6.2583e-02, -2.3051e-01,  2.6022e-02],\n",
      "         [ 2.3316e-01,  4.4131e-02,  3.6673e-01,  1.4711e-02,  1.0517e-04,\n",
      "          -6.6118e-02,  2.0641e-01, -1.1268e-01, -2.4833e-01,  7.9360e-02],\n",
      "         [ 5.0494e-01,  3.0567e-02,  4.7514e-02, -1.0566e-01, -3.6999e-01,\n",
      "          -1.5165e-01, -1.1815e-01, -1.6956e-01,  4.3213e-02, -1.4625e-01],\n",
      "         [ 4.1134e-01, -1.3207e-01,  2.6191e-01, -2.1619e-01,  5.1732e-02,\n",
      "          -2.4118e-01,  5.9468e-02, -2.1053e-01, -1.3901e-01,  5.4877e-02]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Combine heads and apply final linear transformation\n",
    "output = output.transpose(1, 2).contiguous().view(N, -1, heads * head_dim)\n",
    "# The heads are combined by transposing and reshaping the output to merge the heads\n",
    "\n",
    "fc_out = nn.Linear(heads * head_dim, embedding_size)\n",
    "# A final linear layer is applied to project the combined heads back to the original embedding size\n",
    "\n",
    "output = fc_out(output)\n",
    "\n",
    "print(\"Final Output Shape:\", output.shape)\n",
    "print(\"Final Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
