{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Embedding Layers in PyTorch\n",
    "\n",
    "This notebook explains the embedding process in PyTorch using an example with a large vocabulary size and embedding dimension.\n",
    "\n",
    "## What is an Embedding Layer?\n",
    "\n",
    "An embedding layer maps discrete input tokens (like words) into continuous vectors. It is used to represent words in a dense vector space where each word has its own unique vector representation.\n",
    "\n",
    "### Why Use Embeddings?\n",
    "- **Dimensionality Reduction:** Instead of one-hot encoding with a large dimension, embeddings provide a lower-dimensional representation.\n",
    "- **Semantic Meaning:** Vectors can capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Explanation\n",
    "\n",
    "### Step 1: Define the Vocabulary\n",
    "For simplicity, we use a small subset of words to illustrate the process. In practice, the vocabulary can be much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a small vocabulary for illustration\n",
    "vocab = {'<pad>': 0, '<unk>': 1, 'the': 2, 'to': 3, 'and': 4, 'of': 5, 'a': 6, 'in': 7, 'that': 8, 'is': 9, 'cat': 10, 'hat': 11}\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Embedding Layer\n",
    "The embedding layer is initialized with a random matrix of size `(vocab_size, embedding_dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding dimension\n",
    "embedding_dim = 512\n",
    "\n",
    "# Initialize the embedding layer\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Embedding Matrix Initialization\n",
    "The embedding matrix \\( E \\) has dimensions `(vocab_size, embedding_dim)`. Each row represents a word vector in the embedding space.\n",
    "\n",
    "### Math Behind Embedding Initialization\n",
    "For each word in the vocabulary, a unique vector of size `embedding_dim` is initialized. This matrix is updated during training to capture the relationships between words.\n",
    "\n",
    "Let's initialize and inspect the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 512])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the size of the embedding matrix\n",
    "embedding_matrix = embedding_layer.weight.data\n",
    "embedding_matrix.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Input Sequence to Embeddings\n",
    "Given an input sequence of token indices, the embedding layer retrieves the corresponding vectors from the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input sequence: \"the cat in the hat\"\n",
    "input_indices = torch.tensor([2, 10, 7, 2, 11], dtype=torch.long)  # Example indices based on the vocab\n",
    "\n",
    "# Get the embeddings for the input sequence\n",
    "embeddings = embedding_layer(input_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect the Embeddings\n",
    "Each token index in the input sequence is mapped to its corresponding 512-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7316,  0.7371, -1.0426,  ..., -0.1416,  0.6958,  0.7521],\n",
       "        [-1.2539, -0.7819,  0.6867,  ..., -0.2595,  1.5378,  0.9113],\n",
       "        [ 1.0973,  1.1544,  0.2164,  ...,  1.4280,  1.2353, -1.2412],\n",
       "        [-0.7316,  0.7371, -1.0426,  ..., -0.1416,  0.6958,  0.7521],\n",
       "        [-0.2964,  1.1866, -1.6716,  ...,  0.2394, -1.0284,  0.8791]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Embeddings\n",
    "- Each row in the output corresponds to a token in the input sequence.\n",
    "- The values in each row are the learned weights representing the token in the 512-dimensional space.\n",
    "- These embeddings capture the semantic meaning and relationships between words.\n",
    "\n",
    "For example, words that are similar or often appear in similar contexts will have similar vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "1. **Vocabulary Size:** The number of unique tokens in the vocabulary (e.g., 10,000).\n",
    "2. **Embedding Dimension:** The size of the vector representing each token (e.g., 512).\n",
    "3. **Embedding Matrix:** A matrix of size `(vocab_size, embedding_dim)` initialized randomly.\n",
    "4. **Token Indices:** Each word in the input sequence is converted to its corresponding index.\n",
    "5. **Embedding Lookup:** The embedding layer retrieves the vectors for the input token indices, resulting in a sequence of vectors.\n",
    "\n",
    "This process allows the model to learn and represent words in a continuous vector space, capturing their semantic relationships and improving the model's performance in tasks like text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Passed to Positional Encoder\n",
    "\n",
    "In a Transformer architecture, after converting tokens to embeddings, the embeddings are passed through the positional encoder. This step adds positional information to the embeddings, enabling the model to understand the order of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More in-depth look at Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['This', 'script', 'implements', 'a', 'decoder-only', 'Transformer', 'model', 'for', 'text', 'generation,', 'similar', 'to', 'the', 'architecture', 'used', 'in', 'GPT', '(Generative', 'Pre-trained', 'Transformer).', 'The', 'model', 'is', 'trained', 'on', 'a', 'text', 'dataset,', 'specifically', 'the', 'book', '\"Pride', 'and', 'Prejudice,\"', 'to', 'learn', 'to', 'generate', 'text', 'in', 'the', 'style', 'of', 'the', 'book.', 'The', 'script', 'trains', 'the', 'model', 'on', 'the', 'text', 'of', '\"Pride', 'and', 'Prejudice\"', 'to', 'generate', 'text', 'in', 'a', 'similar', 'style.', 'After', 'training,', 'the', 'model', 'can', 'be', 'used', 'to', 'generate', 'text', 'by', 'predicting', 'the', 'next', 'token', 'in', 'a', 'sequence', 'based', 'on', 'the', 'previous', 'tokens.']\n",
      "Unique Words: ['This', 'script', 'implements', 'a', 'decoder-only', 'Transformer', 'model', 'for', 'text', 'generation,', 'similar', 'to', 'the', 'architecture', 'used', 'in', 'GPT', '(Generative', 'Pre-trained', 'Transformer).', 'The', 'model', 'is', 'trained', 'on', 'a', 'text', 'dataset,', 'specifically', 'the', 'book', '\"Pride', 'and', 'Prejudice,\"', 'to', 'learn', 'to', 'generate', 'text', 'in', 'the', 'style', 'of', 'the', 'book.', 'The', 'script', 'trains', 'the', 'model', 'on', 'the', 'text', 'of', '\"Pride', 'and', 'Prejudice\"', 'to', 'generate', 'text', 'in', 'a', 'similar', 'style.', 'After', 'training,', 'the', 'model', 'can', 'be', 'used', 'to', 'generate', 'text', 'by', 'predicting', 'the', 'next', 'token', 'in', 'a', 'sequence', 'based', 'on', 'the', 'previous', 'tokens.']\n",
      "Vocab: {'trains': 4, 'a': 5, 'script': 6, 'can': 7, 'decoder-only': 8, 'next': 9, '\"Pride': 10, 'After': 11, 'used': 12, 'the': 13, 'on': 14, 'implements': 15, 'dataset,': 16, 'to': 17, 'tokens.': 18, 'is': 19, 'architecture': 20, 'This': 21, '(Generative': 22, 'Pre-trained': 23, 'text': 24, 'sequence': 25, 'similar': 26, 'Prejudice,\"': 27, 'and': 28, 'learn': 29, 'style': 30, 'previous': 31, 'of': 32, 'in': 33, 'for': 34, 'Transformer).': 35, 'The': 36, 'trained': 37, 'training,': 38, 'by': 39, 'book.': 40, 'style.': 41, 'be': 42, 'predicting': 43, 'specifically': 44, 'Prejudice\"': 45, 'GPT': 46, 'token': 47, 'based': 48, 'generation,': 49, 'model': 50, 'book': 51, 'generate': 52, 'Transformer': 53}\n",
      "Vocabulary size: 54\n",
      "Sample vocabulary: {'trains': 4, 'a': 5, 'script': 6, 'can': 7, 'decoder-only': 8, 'next': 9, '\"Pride': 10, 'After': 11, 'used': 12, 'the': 13}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"\n",
    "This script implements a decoder-only Transformer model for text generation, similar to the architecture used in GPT (Generative Pre-trained Transformer). The model is trained on a text dataset, specifically the book \"Pride and Prejudice,\" to learn to generate text in the style of the book.\n",
    "The script trains the model on the text of \"Pride and Prejudice\" to generate text in a similar style. After training, the model can be used to generate text by predicting the next token in a sequence based on the previous tokens.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text and create the vocabulary\n",
    "def create_vocab(text):\n",
    "    words = text.split()\n",
    "    print(f\"Words: {words}\")\n",
    "    unique_words = set(words) # removes duplicates\n",
    "    print(f\"Unique Words: {words}\")\n",
    "    vocab = {word: i + 4 for i, word in enumerate(unique_words)} # create dict - map each word to a index\n",
    "    print(f\"Vocab: {vocab}\")\n",
    "\n",
    "    # Add special tokens to the vocabulary with fixed indices\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<unk>'] = 1\n",
    "    vocab['<sos>'] = 2\n",
    "    vocab['<eos>'] = 3\n",
    "    return vocab\n",
    "\n",
    "vocab = create_vocab(text)\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Sample vocabulary:\", {k: vocab[k] for k in list(vocab)[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 54\n",
      "Embedding Matrix Share: Embedding(54, 10)\n",
      "Embedding Matrix:\n",
      "tensor([[ 7.3596e-01,  1.2562e+00, -1.3335e+00,  2.5319e-01, -4.4919e-01,\n",
      "          1.7723e+00, -1.5894e+00,  3.8627e-01,  2.4122e-01,  1.6544e+00],\n",
      "        [-1.0111e+00,  4.1950e-01, -4.4773e-01, -1.7319e-01, -5.7057e-01,\n",
      "          1.8018e-01, -1.1488e+00, -1.8112e+00,  3.3982e-01, -5.2644e-02],\n",
      "        [ 1.7883e+00, -1.3003e+00,  1.4804e-01, -7.5168e-01,  4.7431e-01,\n",
      "         -6.3050e-01, -9.9474e-01, -5.7372e-02,  1.9889e-01, -3.6535e-01],\n",
      "        [ 1.6899e+00, -1.2200e+00, -7.5872e-01, -5.3970e-01, -3.8359e-01,\n",
      "          1.7771e+00,  4.6346e-01, -6.5572e-01,  1.2301e+00,  1.0581e+00],\n",
      "        [-1.9759e-02, -1.5920e-02, -1.3658e-01, -1.5800e+00, -1.7770e+00,\n",
      "          3.2761e-02, -2.2128e-01,  8.9837e-01, -1.7743e+00,  1.1366e-01],\n",
      "        [-9.7560e-01,  5.2428e-01, -1.0213e+00,  2.6234e-01,  1.0089e-01,\n",
      "          8.1758e-01, -1.4203e+00, -8.4550e-02, -1.1753e+00,  1.9893e-01],\n",
      "        [ 7.4582e-02, -1.0402e+00,  1.6724e+00,  2.3068e-01,  4.0047e-02,\n",
      "          1.0399e+00,  3.7405e-01, -5.9152e-01,  1.3554e+00, -2.3176e+00],\n",
      "        [-3.1259e-01,  1.2778e-01, -5.8559e-02, -9.5754e-01,  1.0868e+00,\n",
      "          1.9624e-01,  5.9325e-01,  5.1297e-01,  1.7633e+00,  6.5696e-01],\n",
      "        [ 3.5868e-01, -7.4149e-02, -1.8580e+00,  1.1830e+00, -3.4019e-01,\n",
      "         -9.5379e-01,  1.9216e+00, -3.9155e-01,  2.0698e-01,  5.7388e-01],\n",
      "        [-9.9368e-01, -3.1582e-01,  1.0002e-01,  1.3667e-01,  1.3011e+00,\n",
      "         -2.7367e-01, -3.1606e-01, -9.7247e-01,  5.4140e-01,  2.2197e-01],\n",
      "        [-1.5824e+00,  1.3133e+00,  6.4992e-01, -1.3901e+00, -3.4190e-01,\n",
      "          1.3243e+00, -1.1464e+00,  1.0701e-01, -1.9144e+00, -4.2266e-01],\n",
      "        [ 9.6538e-01, -1.0309e+00, -9.9225e-01, -5.2017e-02, -4.5365e-01,\n",
      "         -8.3894e-01,  1.3881e+00, -7.7209e-01,  1.9112e-01,  7.9556e-01],\n",
      "        [-7.1231e-02,  2.3125e-01,  4.9596e-01,  1.6915e+00, -7.0208e-01,\n",
      "          8.0569e-02, -2.6724e-01,  6.0004e-01, -6.1903e-01,  3.4793e-01],\n",
      "        [ 1.1392e+00, -6.0768e-01, -8.7103e-01, -1.9851e+00,  2.5643e-01,\n",
      "          1.6157e-01,  5.9326e-02, -8.6455e-01,  8.2807e-01,  6.2443e-01],\n",
      "        [-5.5182e-01, -2.6011e-01,  2.6257e-01,  1.1867e+00,  3.9078e-01,\n",
      "         -1.2808e+00, -1.2179e+00, -1.0198e+00, -2.5192e-01,  7.7329e-01],\n",
      "        [-1.2554e+00,  6.6871e-01,  1.0026e-01, -9.1042e-01,  1.5644e+00,\n",
      "          1.6488e+00, -1.9674e-01, -1.6907e+00, -4.0793e-01,  2.8987e-01],\n",
      "        [ 3.8113e-01, -2.9487e-02,  6.6488e-01,  3.8052e-01, -7.4962e-01,\n",
      "          2.4418e-01, -8.3822e-01,  1.4271e+00, -1.7747e-01,  6.9221e-01],\n",
      "        [-6.2388e-01,  8.2851e-01,  1.1041e-01, -2.5575e+00,  2.9968e-01,\n",
      "         -1.5813e+00, -1.4498e+00,  1.4838e+00,  1.1247e+00, -4.5763e-01],\n",
      "        [ 1.2496e+00,  6.3462e-01,  9.0873e-01, -9.4112e-02,  9.4191e-01,\n",
      "          7.4928e-01, -6.6397e-01, -1.3164e-01, -1.3448e+00,  2.7832e-01],\n",
      "        [-8.5830e-01, -7.1767e-01, -9.1245e-01, -2.0715e-01, -2.6448e+00,\n",
      "         -1.4396e+00,  2.5143e-01,  1.2954e+00, -1.7585e+00,  9.7546e-01],\n",
      "        [ 8.0182e-01,  4.5093e-01,  8.5640e-02,  2.8823e-01,  5.8665e-01,\n",
      "         -1.0953e+00,  1.9564e+00,  1.8447e-01, -1.2071e+00,  6.0226e-01],\n",
      "        [ 1.7122e-02,  6.5356e-02,  4.6164e-01, -8.0396e-02,  7.1929e-01,\n",
      "         -1.7430e+00,  7.1195e-01, -5.8019e-01,  2.7427e+00, -7.1563e-01],\n",
      "        [ 5.9784e-01,  2.3649e+00, -1.5434e+00, -1.5868e+00, -1.5643e+00,\n",
      "         -9.1219e-01, -1.4620e+00, -4.8105e-02,  4.3787e-01, -6.5199e-02],\n",
      "        [ 6.3120e-01, -6.4131e-01, -4.9551e-01,  1.7949e+00,  9.6149e-01,\n",
      "          5.8945e-01,  6.6659e-01,  1.6704e+00,  8.2516e-01,  4.6444e-01],\n",
      "        [ 6.0083e-01, -7.0607e-01,  2.3409e+00, -8.1299e-01, -1.9791e-02,\n",
      "          8.7234e-01, -1.0504e+00, -3.5106e-01,  9.8820e-01, -5.9697e-01],\n",
      "        [ 7.8606e-02, -4.9734e-01, -1.3229e+00, -1.0642e+00,  2.0038e+00,\n",
      "          1.5647e+00,  7.7804e-01, -8.4726e-02,  1.3971e-01,  3.1481e-01],\n",
      "        [-2.7242e-01,  1.6542e+00,  1.4321e+00,  4.7101e-01,  1.4433e-01,\n",
      "         -6.3565e-01, -1.6510e+00, -6.5232e-01,  4.7448e-02,  1.2610e+00],\n",
      "        [ 2.7860e-02, -1.4120e-01, -1.0089e+00, -9.0911e-01, -1.4398e+00,\n",
      "         -8.7560e-01, -6.9715e-01, -2.0825e+00, -2.2735e-01, -4.9745e-01],\n",
      "        [-7.5017e-01,  6.8477e-01, -4.7862e-01, -7.6713e-01, -9.8239e-01,\n",
      "          1.3912e+00,  8.0554e-02,  1.1475e+00, -8.0365e-01,  3.3347e-01],\n",
      "        [-2.3095e-01, -8.4611e-01, -6.8415e-01,  8.8221e-01,  1.2988e-01,\n",
      "         -1.5332e-01,  4.7082e-01, -2.6064e-01, -4.4696e-01, -1.2489e+00],\n",
      "        [ 2.3567e+00, -3.3114e+00,  7.3255e-01,  7.9768e-01, -1.6513e+00,\n",
      "         -2.0678e-01,  1.6207e+00, -1.0308e+00, -1.4201e+00,  2.6627e-01],\n",
      "        [ 1.7774e+00,  8.9775e-01, -7.2123e-02,  9.1161e-01, -2.7543e+00,\n",
      "         -6.5766e-01,  4.7032e-01, -1.2187e+00, -5.6738e-02,  3.9729e-02],\n",
      "        [-2.0380e+00,  5.1715e-01,  1.0243e+00, -3.2988e-01, -2.1583e-01,\n",
      "          1.6724e+00,  3.6286e-01, -9.0102e-01, -1.0884e+00,  7.4523e-01],\n",
      "        [ 2.4560e-01, -1.4707e+00, -1.2636e+00,  1.0315e+00, -6.4968e-01,\n",
      "         -6.0441e-01,  1.4999e-01, -1.6375e+00,  5.0096e-01, -4.4623e-01],\n",
      "        [-1.0120e+00,  1.3493e+00,  9.7363e-01,  2.7219e-01, -1.9441e+00,\n",
      "         -1.6458e-01, -9.8986e-01,  8.3292e-01,  2.9650e-01,  8.3357e-03],\n",
      "        [-4.5980e-01, -6.6303e-01,  4.0393e-01, -5.3414e-01,  1.1526e+00,\n",
      "          4.8083e-01, -1.2882e-01,  3.6985e-01,  8.7812e-02, -7.4777e-01],\n",
      "        [ 3.7409e-01,  3.0544e-01, -2.2740e-01, -6.0392e-01, -9.0505e-01,\n",
      "          1.3377e+00, -7.3062e-01,  6.9384e-01, -1.8271e+00, -6.6076e-02],\n",
      "        [ 2.3154e+00, -6.7124e-01, -3.7395e-01,  4.2727e-01,  1.3057e-01,\n",
      "         -3.2953e-01, -6.8106e-01,  3.0230e+00,  2.4060e-01, -1.3090e+00],\n",
      "        [-2.6278e-01,  2.7758e-01,  6.2763e-01,  5.6323e-01, -8.4323e-01,\n",
      "          8.7014e-01, -4.6516e-01,  7.7609e-01,  3.1204e-01,  9.9203e-01],\n",
      "        [-3.6106e-01, -1.8483e+00, -8.0889e-01,  1.0717e+00,  3.8182e-01,\n",
      "         -9.4247e-01, -2.3747e+00, -7.5161e-01,  4.3231e-01, -3.7623e-01],\n",
      "        [-9.0622e-01, -1.8998e+00,  2.5066e-02,  5.0025e-02,  1.3428e-01,\n",
      "         -4.8517e-01,  6.0419e-02, -5.8232e-01, -3.1846e+00,  1.2306e+00],\n",
      "        [-2.7947e-01, -1.7808e+00,  2.8221e-01,  2.2049e-01,  5.1156e-01,\n",
      "          1.1786e-01,  1.7110e+00, -8.7852e-02,  3.7174e-01, -1.5630e+00],\n",
      "        [ 1.4814e+00, -7.5899e-02, -1.8380e+00,  1.4146e+00,  1.1331e+00,\n",
      "          1.4390e+00, -7.9303e-01,  3.1949e-01,  3.7877e-01, -4.2854e-01],\n",
      "        [ 3.3505e-01,  8.3840e-02, -5.3736e-01, -1.7596e-01,  9.7267e-01,\n",
      "         -1.3355e+00, -1.0830e+00,  5.6247e-01,  8.7481e-02, -5.9278e-01],\n",
      "        [-5.0642e-01, -2.0630e-01, -9.3925e-01,  8.2994e-01, -1.0236e-01,\n",
      "         -5.4954e-01,  7.1617e-01,  4.0161e-01,  1.0227e+00,  8.8250e-01],\n",
      "        [-4.8750e-01, -1.4020e+00, -5.2938e-01,  9.8735e-01, -8.9287e-01,\n",
      "         -7.5649e-01, -3.1334e-01, -1.2966e-01, -1.2433e+00,  2.3897e-01],\n",
      "        [-1.2682e+00,  5.8464e-01, -7.7391e-01,  2.7209e-03,  3.5202e-01,\n",
      "         -2.1088e+00,  9.3687e-02,  1.2218e+00, -2.5730e-01, -9.7281e-01],\n",
      "        [ 1.9222e+00, -1.1294e-02,  1.1329e-01, -2.7988e-01, -1.4592e-01,\n",
      "         -1.5613e-01, -1.3650e+00, -5.7233e-01,  2.1385e-02,  4.0263e-01],\n",
      "        [-1.0105e+00,  1.3517e-01,  8.5408e-01,  8.9257e-01, -1.1432e+00,\n",
      "         -8.8237e-01,  3.1446e-01,  7.4968e-01,  1.0580e+00, -3.9348e-01],\n",
      "        [-2.3607e-01,  8.6531e-02, -7.8989e-01, -4.3666e-01,  9.6588e-01,\n",
      "          8.5399e-01,  1.3754e+00, -9.0249e-01, -1.6444e+00,  2.6953e+00],\n",
      "        [ 9.7863e-01, -2.0795e+00, -2.4983e-01,  8.0523e-01, -6.6857e-01,\n",
      "         -3.1857e-01, -5.5915e-01,  8.0161e-01,  7.8622e-01, -1.9430e+00],\n",
      "        [ 1.1945e-01, -2.2162e-01, -1.0895e-01,  1.4418e+00,  9.2171e-01,\n",
      "         -2.6973e-01,  2.5381e+00, -6.8749e-01,  1.1547e+00,  6.8016e-01],\n",
      "        [ 8.9777e-01, -4.8876e-01, -9.7612e-01,  1.0175e-01,  2.2763e-01,\n",
      "          4.5039e-01, -4.2176e-01, -7.2506e-01,  2.3221e-01,  1.8532e+00],\n",
      "        [-5.6510e-01,  9.5218e-01, -1.5254e-01, -8.5498e-01, -5.3167e-01,\n",
      "         -1.0827e+00, -1.6945e+00, -7.5183e-01, -2.3964e-01, -1.5830e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding layer\n",
    "vocab_size = len(vocab)  # Example vocabulary size\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "\n",
    "embedding_dim = 10  # Embedding dimension\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "print(f\"Embedding Matrix Share: {embedding_layer}\")\n",
    "\n",
    "# Extract the embedding matrix\n",
    "embedding_matrix = embedding_layer.weight.data\n",
    "\n",
    "print(\"Embedding Matrix:\")\n",
    "print(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding matrix of shape (54, 10) is created. Each row in this matrix corresponds to a word vector of size 10.  We can also print out the full Embedding Matrix as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the First Sentence and Convert to Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence indicies: [21, 6, 15, 5, 8, 53, 50, 34, 24, 1]\n",
      "Input indices: tensor([21,  6, 15,  5,  8, 53, 50, 34, 24,  1])\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"This script implements a decoder-only Transformer model for text generation\"\n",
    "first_sentence_indices = [vocab.get(word, vocab['<unk>']) for word in first_sentence.split()]\n",
    "print(f\"Sentence indicies: {first_sentence_indices}\")\n",
    "\n",
    "# Turn it into a tensor for processing\n",
    "input_indices = torch.tensor(first_sentence_indices, dtype=torch.long)\n",
    "print(f\"Input indices: {input_indices}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the embeddings for the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_layer(input_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the corresponding rows from the embedding matrix  E  for each index.\n",
    "\n",
    "So, If first_sentence_indices = [21, 6, 15, 8, 53, 50, 34, 24, 1], the corresponding embeddings would be something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_example = [\n",
    " [0.5, 0.6, ..., 0.8],  # Embedding for 'This' (index 21)\n",
    " [0.3, 0.4, ..., 0.7],  # Embedding for 'script' (index 6)\n",
    " [0.7, 0.1, ..., 0.9],  # Embedding for 'implements' (index 15)\n",
    " [0.4, 0.5, ..., 0.6],  # Embedding for 'decoder-only' (index 8)\n",
    " [0.2, 0.3, ..., 0.4],  # Embedding for 'Transformer' (index 53)\n",
    " [0.4, 0.5, ..., 0.6],  # Embedding for 'model' (index 50)\n",
    " [0.1, 0.2, ..., 0.3],  # Embedding for 'for' (index 34)\n",
    " [0.3, 0.4, ..., 0.7],  # Embedding for 'text' (index 24)\n",
    " [0.3, 0.4, ..., 0.7],  # Embedding for 'generation' (<unk>, index 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's get the actualy Embeddings from the first_sentence_indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape:  torch.Size([10, 10])\n",
      "\n",
      "first_sentence_indices Embeddings: \n",
      "\n",
      "tensor([[ 0.0171,  0.0654,  0.4616, -0.0804,  0.7193, -1.7430,  0.7120, -0.5802,\n",
      "          2.7427, -0.7156],\n",
      "        [ 0.0746, -1.0402,  1.6724,  0.2307,  0.0400,  1.0399,  0.3740, -0.5915,\n",
      "          1.3554, -2.3176],\n",
      "        [-1.2554,  0.6687,  0.1003, -0.9104,  1.5644,  1.6488, -0.1967, -1.6907,\n",
      "         -0.4079,  0.2899],\n",
      "        [-0.9756,  0.5243, -1.0213,  0.2623,  0.1009,  0.8176, -1.4203, -0.0846,\n",
      "         -1.1753,  0.1989],\n",
      "        [ 0.3587, -0.0741, -1.8580,  1.1830, -0.3402, -0.9538,  1.9216, -0.3916,\n",
      "          0.2070,  0.5739],\n",
      "        [-0.5651,  0.9522, -0.1525, -0.8550, -0.5317, -1.0827, -1.6945, -0.7518,\n",
      "         -0.2396, -0.1583],\n",
      "        [ 0.9786, -2.0795, -0.2498,  0.8052, -0.6686, -0.3186, -0.5592,  0.8016,\n",
      "          0.7862, -1.9430],\n",
      "        [-1.0120,  1.3493,  0.9736,  0.2722, -1.9441, -0.1646, -0.9899,  0.8329,\n",
      "          0.2965,  0.0083],\n",
      "        [ 0.6008, -0.7061,  2.3409, -0.8130, -0.0198,  0.8723, -1.0504, -0.3511,\n",
      "          0.9882, -0.5970],\n",
      "        [-1.0111,  0.4195, -0.4477, -0.1732, -0.5706,  0.1802, -1.1488, -1.8112,\n",
      "          0.3398, -0.0526]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings shape: \", embeddings.shape)\n",
    "\n",
    "print(f\"\\nfirst_sentence_indices Embeddings: \\n\\n{embeddings}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
