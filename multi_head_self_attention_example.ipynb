{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bffc82",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding Multi-Head Self-Attention in Transformers\n",
    "\n",
    "This notebook provides a detailed step-by-step explanation of the multi-head self-attention mechanism in Transformer models. We will use a simple example to illustrate the computations involved, including linear transformations, scaled dot-product attention, and the final concatenation and linear transformation.\n",
    "\n",
    "### Example Input\n",
    "\n",
    "Let's assume we have a simple input sequence of three tokens, represented as embeddings:\n",
    "- Embedding size: 6\n",
    "- Number of heads: 2\n",
    "\n",
    "The steps involved in the multi-head self-attention mechanism are:\n",
    "\n",
    "1. Linear transformations for Queries, Keys, and Values\n",
    "2. Splitting into multiple heads\n",
    "3. Scaled dot-product attention\n",
    "4. Concatenation of heads and final linear transformation\n",
    "\n",
    "We will illustrate each of these steps in detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e1630",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Linear Transformations for Queries, Keys, and Values\n",
    "\n",
    "Each input embedding is transformed into queries, keys, and values using learned weight matrices.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's assume we have the following embedding matrix for a sequence of three tokens (each of size 6):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146cf096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries:\n",
      " [[1.2 1.5 1.8]\n",
      " [2.4 3.  3.6]\n",
      " [2.4 3.  3.6]]\n",
      "Keys:\n",
      " [[1.2 1.5 1.8]\n",
      " [2.4 3.  3.6]\n",
      " [2.4 3.  3.6]]\n",
      "Values:\n",
      " [[1.2 1.5 1.8]\n",
      " [2.4 3.  3.6]\n",
      " [2.4 3.  3.6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Example input embeddings (3 tokens, each of size 6)\n",
    "X = np.array([[1, 0, 1, 0, 1, 0],\n",
    "              [0, 2, 0, 2, 0, 2],\n",
    "              [1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "# Learned weight matrices for queries, keys, and values (for simplicity, we use smaller matrices)\n",
    "W_Q = np.array([[0.1, 0.2, 0.3],\n",
    "                [0.4, 0.5, 0.6],\n",
    "                [0.7, 0.8, 0.9],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.4, 0.5, 0.6],\n",
    "                [0.7, 0.8, 0.9]])\n",
    "\n",
    "W_K = W_Q  # For simplicity, using the same matrix for keys\n",
    "W_V = W_Q  # For simplicity, using the same matrix for values\n",
    "\n",
    "# Linear transformations\n",
    "Q = np.dot(X, W_Q)\n",
    "K = np.dot(X, W_K)\n",
    "V = np.dot(X, W_V)\n",
    "\n",
    "print(\"Queries:\\n\", Q)\n",
    "print(\"Keys:\\n\", K)\n",
    "print(\"Values:\\n\", V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07366d2c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Splitting into Multiple Heads\n",
    "\n",
    "We split the resulting queries, keys, and values into multiple heads. Each head will have a dimension of 3 (embedding size / number of heads).\n",
    "\n",
    "### Example\n",
    "\n",
    "Splitting the queries, keys, and values into 2 heads:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32afebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```python\n",
    "# Splitting into 2 heads (each of size 3)\n",
    "def split_heads(X, num_heads):\n",
    "    batch_size, seq_length, embed_size = X.shape\n",
    "    head_dim = embed_size // num_heads\n",
    "    X = X.reshape(batch_size, seq_length, num_heads, head_dim)\n",
    "    X = X.transpose(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "    return X\n",
    "\n",
    "num_heads = 2\n",
    "Q_heads = split_heads(Q, num_heads)\n",
    "K_heads = split_heads(K, num_heads)\n",
    "V_heads = split_heads(V, num_heads)\n",
    "\n",
    "print(\"Q_heads shape:\", Q_heads.shape)\n",
    "print(\"K_heads shape:\", K_heads.shape)\n",
    "print(\"V_heads shape:\", V_heads.shape)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f348f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Scaled Dot-Product Attention\n",
    "\n",
    "For each head, we compute the attention scores and weighted sums.\n",
    "\n",
    "### Example\n",
    "\n",
    "Compute the attention scores for one of the heads:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627887b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```python\n",
    "# Scaled dot-product attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / math.sqrt(d_k)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    output = np.matmul(weights, V)\n",
    "    return output\n",
    "\n",
    "# Compute attention for each head\n",
    "attention_outputs = [scaled_dot_product_attention(Q_heads[:, i], K_heads[:, i], V_heads[:, i]) for i in range(num_heads)]\n",
    "\n",
    "print(\"Attention outputs for each head:\")\n",
    "for i, attn_output in enumerate(attention_outputs):\n",
    "    print(f\"Head {i+1}:\\n\", attn_output)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3157fc",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Concatenation of Heads and Linear Transformation\n",
    "\n",
    "After computing the attention output for each head, we concatenate the outputs and apply a final linear transformation.\n",
    "\n",
    "### Example\n",
    "\n",
    "Concatenate the outputs and apply a linear transformation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```python\n",
    "# Concatenate outputs from each head\n",
    "def concatenate_heads(outputs):\n",
    "    batch_size, num_heads, seq_length, head_dim = outputs[0].shape\n",
    "    concatenated = np.concatenate(outputs, axis=-1)  # (batch_size, seq_length, num_heads * head_dim)\n",
    "    return concatenated\n",
    "\n",
    "# Linear transformation after concatenation (for simplicity, using an identity matrix)\n",
    "W_O = np.eye(num_heads * Q_heads.shape[-1])\n",
    "concatenated = concatenate_heads(attention_outputs)\n",
    "final_output = np.dot(concatenated, W_O)\n",
    "\n",
    "print(\"Concatenated output shape:\", concatenated.shape)\n",
    "print(\"Final output:\\n\", final_output)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
